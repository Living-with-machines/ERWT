{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0f9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import DataCollatorWithPadding\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b07302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr'],\n",
       "    num_rows: 11315511\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = load_from_disk('/datadrive_2/')\n",
    "#test_data = dataset['test']\n",
    "\n",
    "cache_dir = '/datadrive_2/hf_cache/'\n",
    "dataset = load_from_disk(\"/datadrive_2/HMD_chunked_100_test/\")\n",
    "dataset #= dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a047ed",
   "metadata": {},
   "source": [
    "## Classify by Political Leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0f1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_pattern = re.compile(r'\\bliberal|\\bconservat|\\btory\\b|\\btories\\b',re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69dea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liberal', 'conservat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_pattern.findall('liberal governments do not fire their conservative political ministers minister')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2875c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sent_split(x):\n",
    "#      return {'data': [\n",
    "#                 {'sentence':s.lower(),\n",
    "#                  'length': len(s.split()),\n",
    "#                  'pol': p, 'loc':l, 'year':y, 'ocr':o,'nlp':n} \n",
    "#                      for y,p,l,o,n,t in zip(x['year'],x['pol'],x['loc'],x['ocr_quality_mean'],x['nlp'],x['text']) \n",
    "#                       for s in sent_tokenize(t) \n",
    "#                          if pol_pattern.findall(s)\n",
    "#                  ]\n",
    "#             }\n",
    "\n",
    "# test_data = dataset.map(sent_split,batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db872537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-6f3d8b9a1021878e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-24d2a5525e368df0.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-2d79338e1133aae6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-b3061892df3a5bbe.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-06bf12bcec6754fc.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-a090791e087fa494.arrow\n",
      "Loading cached shuffled indices for dataset at /datadrive_2/HMD_chunked_100_test/cache-2e3366e450e9f2f2.arrow\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset.map(\n",
    "                lambda x: {'sentences': x['sentences'].lower()}, num_proc=6\n",
    "                        ).shuffle(seed=42).select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c0086b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr'],\n",
       "    num_rows: 15000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3edfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = test_data.filter(lambda x: x['data.length'] > 25).shuffle(seed=42).select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d45648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-ff0fd48ef7c4aa88.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-6aeec3fc41d9a975.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-0f45e64b62b46afe.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-a1b252da72d25d12.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-4bbfb9a6d353944e.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-b1a1fc69d546bff6.arrow\n"
     ]
    }
   ],
   "source": [
    "# def pred_data(example,add_field='year'):\n",
    "#     return {'st_year_sep': f'[{example[add_field]}]' + ' [SEP] ' + example['sentences'] ,\n",
    "#      'year_sep': str(example[add_field]) + ' [SEP] ' + example['sentences'] ,\n",
    "#      'year_date': str(example[add_field]) + ' [DATE] ' + example['sentences'],\n",
    "        \n",
    "#     }\n",
    "def pred_data(example):\n",
    "    return {'label':float(example['year'])}\n",
    "    \n",
    "data = test_data.map(pred_data , num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad873104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9989630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a32db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab2code = {'[con]':0,'[lib]':1,'[rad]':2,'[neutr]':3,'[none]':4}\n",
    "# num_labels = len(lab2code)\n",
    "# data = data.map(lambda x: {'label': lab2code[x['pol']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcecd756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 1859,\n",
       " 'nlp': 2084,\n",
       " 'pol': '[neutr]',\n",
       " 'loc': '[liverpool]',\n",
       " 'sentences': 'sitenirranv, roy institute as tou ry, v. ireno. i.—nervous debility, loss of kient.„2,l:oo!op orof sight, prostration of physical energy, 1;\\': asor\\'\\'business, study, or society, with dr. marston \\'01coax. men.o• heiti 0addressed specially to young 14. ...e oh 0%—marrlagerbanodbilgaltetipoltnsodanudetilrn°ooimp ten &c. \\' addressed to those who dam. 1010 . totof removing all impediments. qi thee dirpthaenß:orrrs4 2ar.cll\\'of healthful children, clearly expl.-11:91p .no. 3.—tiie great social evil. 11\\' wee voerdiseases which result from it, with da. minor ~,fc „i;,da. massrosi continues to be consulted pe ,e,:.„penned with.veoftiit.reatment, by which msactinr luso* a\\'\\'\\'s,y.o,,tifi,,:vilti9.residence,ez ens-......r5z 0 d-6 orby letter,47, eitherin at thexrßotal.lnsroittitatetßa: 1171,01\\'\"raftpaordrticeuiveargry.inf°\"\"ti°l4 70-00.',\n",
       " 'ocr': 0.643,\n",
       " 'label': 1859.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4c0efbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /datadrive_2/HMD_chunked_100_test/cache-772543886c89d7f3.arrow and /datadrive_2/HMD_chunked_100_test/cache-e018be346372ac2c.arrow\n",
      "Loading cached split indices for dataset at /datadrive_2/HMD_chunked_100_test/cache-0f7c9122ef09d4a9.arrow and /datadrive_2/HMD_chunked_100_test/cache-4947f263d6c6457a.arrow\n"
     ]
    }
   ],
   "source": [
    "test_size = int(len(data)*.2)\n",
    "train_test = data.train_test_split(test_size=test_size, seed=1984)\n",
    "test_set = train_test['test']\n",
    "val_size = int(len(train_test['train'])*.15)\n",
    "train_val =  train_test['train'].train_test_split(test_size=val_size, seed=1984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f7f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr', 'label'],\n",
       "        num_rows: 10200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr', 'label'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "495b21a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-hmd were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-hmd and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y_masked_25 were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y_masked_25 and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y_masked_75 were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y_masked_75 and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-combined were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-combined and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoints = [('distilbert','distilbert-base-uncased','[SEP]','sentences'),\n",
    "               ('hmd_distilbert','/datadrive_2/bnert-hmd','[SEP]','sentences'),\n",
    "               #('bnert-time-st-y','/datadrive_2/bnert-time-st-y','[SEP]','sentences'),\n",
    "               ('bnert-time-y','/datadrive_2/bnert-time-y','[DATE]','sentences'),\n",
    "               ('bnert-time-y_masked_25','/datadrive_2/bnert-time-y_masked_25','[DATE]','sentences'),\n",
    "               ('bnert-time-y_masked_75','/datadrive_2/bnert-time-y_masked_75','[DATE]','sentences'),\n",
    "               ('bnert-comb','/datadrive_2/bnert-combined','[DATE]','sentences'),\n",
    "               #('bnert-pol-st','/datadrive_2/bnert-pol-st','[SEP]','year_sep'),\n",
    "               #('bnert-pol','/datadrive_2/bnert-pol','[SEP]','sentences')\n",
    "              ]\n",
    "\n",
    "model_dict = defaultdict(dict)\n",
    "for name,checkpoint, st, sent_col in checkpoints:\n",
    "    model_dict[name]['model'] = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=1)\n",
    "    model_dict[name]['tokenizer'] = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    #model_dict[name]['special_token'] = st\n",
    "    model_dict[name]['sentences'] = sent_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44f12775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['year', 'pol', 'sentences', 'label'],\n",
       "        num_rows: 10200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['year', 'pol', 'sentences', 'label'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val = train_val.remove_columns(['nlp', 'ocr', 'loc'])\n",
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6fed10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def add_text_col(example,source):\n",
    "#    return {'text' : example[source]}\n",
    "\n",
    "def preprocess_function(examples, target_col):\n",
    "    return tokenizer(examples[target_col], truncation=True, padding=\"max_length\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a244b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    #print(labels, logits)\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "    single_squared_errors = ((logits - labels).flatten()**2).tolist()\n",
    "    \n",
    "    # Compute accuracy \n",
    "    # Based on the fact that the rounded score = true score only if |single_squared_errors| < 0.5\n",
    "    accuracy = sum([1 for e in single_squared_errors if e < 0.25]) / len(single_squared_errors)\n",
    "    \n",
    "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2, \"accuracy\": accuracy}\n",
    "\n",
    "class RegressionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0][:, 0]\n",
    "        loss = torch.nn.functional.mse_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07e895a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-ad2d056fe0e39963.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-d25d1d2e1389c0e8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for distilbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12750' max='12750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12750/12750 27:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3384735.488000</td>\n",
       "      <td>3380671.250000</td>\n",
       "      <td>3380671.250000</td>\n",
       "      <td>1838.580322</td>\n",
       "      <td>-11505.560588</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3373005.056000</td>\n",
       "      <td>3370549.000000</td>\n",
       "      <td>3370548.500000</td>\n",
       "      <td>1835.825439</td>\n",
       "      <td>-11471.107772</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3363598.592000</td>\n",
       "      <td>3360653.750000</td>\n",
       "      <td>3360653.750000</td>\n",
       "      <td>1833.128418</td>\n",
       "      <td>-11437.428809</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3352548.864000</td>\n",
       "      <td>3351115.750000</td>\n",
       "      <td>3351115.750000</td>\n",
       "      <td>1830.525024</td>\n",
       "      <td>-11404.964619</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3346493.184000</td>\n",
       "      <td>3342276.250000</td>\n",
       "      <td>3342276.250000</td>\n",
       "      <td>1828.108887</td>\n",
       "      <td>-11374.878832</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3336204.544000</td>\n",
       "      <td>3334486.000000</td>\n",
       "      <td>3334486.000000</td>\n",
       "      <td>1825.976929</td>\n",
       "      <td>-11348.362710</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3330088.192000</td>\n",
       "      <td>3328080.250000</td>\n",
       "      <td>3328080.500000</td>\n",
       "      <td>1824.222168</td>\n",
       "      <td>-11326.561296</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3324356.608000</td>\n",
       "      <td>3323312.250000</td>\n",
       "      <td>3323312.750000</td>\n",
       "      <td>1822.915039</td>\n",
       "      <td>-11310.332996</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3319554.816000</td>\n",
       "      <td>3320370.750000</td>\n",
       "      <td>3320370.750000</td>\n",
       "      <td>1822.107788</td>\n",
       "      <td>-11300.319515</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3318539.520000</td>\n",
       "      <td>3319372.500000</td>\n",
       "      <td>3319372.750000</td>\n",
       "      <td>1821.833862</td>\n",
       "      <td>-11296.922783</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../results_distilbert/checkpoint-500\n",
      "Configuration saved in ../results_distilbert/checkpoint-500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-1000\n",
      "Configuration saved in ../results_distilbert/checkpoint-1000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-1500\n",
      "Configuration saved in ../results_distilbert/checkpoint-1500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-2000\n",
      "Configuration saved in ../results_distilbert/checkpoint-2000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-2500\n",
      "Configuration saved in ../results_distilbert/checkpoint-2500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-2500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-3000\n",
      "Configuration saved in ../results_distilbert/checkpoint-3000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-3500\n",
      "Configuration saved in ../results_distilbert/checkpoint-3500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-4000\n",
      "Configuration saved in ../results_distilbert/checkpoint-4000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-4500\n",
      "Configuration saved in ../results_distilbert/checkpoint-4500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-5000\n",
      "Configuration saved in ../results_distilbert/checkpoint-5000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-5000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-5500\n",
      "Configuration saved in ../results_distilbert/checkpoint-5500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-6000\n",
      "Configuration saved in ../results_distilbert/checkpoint-6000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-6500\n",
      "Configuration saved in ../results_distilbert/checkpoint-6500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-7000\n",
      "Configuration saved in ../results_distilbert/checkpoint-7000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-7500\n",
      "Configuration saved in ../results_distilbert/checkpoint-7500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-7500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-8000\n",
      "Configuration saved in ../results_distilbert/checkpoint-8000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-8500\n",
      "Configuration saved in ../results_distilbert/checkpoint-8500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-8500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-9000\n",
      "Configuration saved in ../results_distilbert/checkpoint-9000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-9500\n",
      "Configuration saved in ../results_distilbert/checkpoint-9500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-10000\n",
      "Configuration saved in ../results_distilbert/checkpoint-10000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-10000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-10500\n",
      "Configuration saved in ../results_distilbert/checkpoint-10500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-11000\n",
      "Configuration saved in ../results_distilbert/checkpoint-11000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-11000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-11500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../results_distilbert/checkpoint-11500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-12000\n",
      "Configuration saved in ../results_distilbert/checkpoint-12000/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_distilbert/checkpoint-12500\n",
      "Configuration saved in ../results_distilbert/checkpoint-12500/config.json\n",
      "Model weights saved in ../results_distilbert/checkpoint-12500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-b65ba8da1d8b744f.arrow\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: loc, sentences, nlp, ocr, year, pol. If loc, sentences, nlp, ocr, year, pol are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: loc, sentences, nlp, ocr, year, pol. If loc, sentences, nlp, ocr, year, pol are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Configuration saved in /datadrive_2/distilbert-time-reg/config.json\n",
      "Model weights saved in /datadrive_2/distilbert-time-reg/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/distilbert-time-reg/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/distilbert-time-reg/special_tokens_map.json\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-2c4a708601269864.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-55b4828d2972c0a2.arrow\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for hmd_distilbert\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12750' max='12750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12750/12750 27:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3383764.480000</td>\n",
       "      <td>3379120.750000</td>\n",
       "      <td>3379120.250000</td>\n",
       "      <td>1838.158447</td>\n",
       "      <td>-11500.282754</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3370524.416000</td>\n",
       "      <td>3367639.500000</td>\n",
       "      <td>3367639.500000</td>\n",
       "      <td>1835.032959</td>\n",
       "      <td>-11461.206013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3360025.600000</td>\n",
       "      <td>3356481.500000</td>\n",
       "      <td>3356481.500000</td>\n",
       "      <td>1831.989990</td>\n",
       "      <td>-11423.226926</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3347558.400000</td>\n",
       "      <td>3345783.500000</td>\n",
       "      <td>3345783.250000</td>\n",
       "      <td>1829.068115</td>\n",
       "      <td>-11386.815793</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3340684.800000</td>\n",
       "      <td>3335889.000000</td>\n",
       "      <td>3335889.250000</td>\n",
       "      <td>1826.361084</td>\n",
       "      <td>-11353.138967</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3329259.264000</td>\n",
       "      <td>3327194.500000</td>\n",
       "      <td>3327194.500000</td>\n",
       "      <td>1823.979248</td>\n",
       "      <td>-11323.545227</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3322481.664000</td>\n",
       "      <td>3320053.000000</td>\n",
       "      <td>3320053.250000</td>\n",
       "      <td>1822.020508</td>\n",
       "      <td>-11299.238959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3315905.280000</td>\n",
       "      <td>3314732.000000</td>\n",
       "      <td>3314731.750000</td>\n",
       "      <td>1820.560059</td>\n",
       "      <td>-11281.127890</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3310819.840000</td>\n",
       "      <td>3311448.750000</td>\n",
       "      <td>3311448.750000</td>\n",
       "      <td>1819.658081</td>\n",
       "      <td>-11269.953188</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3309597.184000</td>\n",
       "      <td>3310335.250000</td>\n",
       "      <td>3310334.750000</td>\n",
       "      <td>1819.351685</td>\n",
       "      <td>-11266.162435</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-1000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-1000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-1500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-1500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-2000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-2000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-2500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-2500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-2500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-3000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-3000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-3500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-3500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-4000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-4000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-4500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-4500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-5000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-5000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-5000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-5500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-5500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-6000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-6000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-6500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-6500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-7000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-7000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-7500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-7500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-7500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-8000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-8000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-8500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-8500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-8500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-9000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-9000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-9500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-9500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-10000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-10000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-10000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-10500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-10500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-11000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-11000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-11000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-11500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-11500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-12000\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-12000/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_hmd_distilbert/checkpoint-12500\n",
      "Configuration saved in ../results_hmd_distilbert/checkpoint-12500/config.json\n",
      "Model weights saved in ../results_hmd_distilbert/checkpoint-12500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-0e3eed62b4d44f6a.arrow\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: loc, sentences, nlp, ocr, year, pol. If loc, sentences, nlp, ocr, year, pol are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: loc, sentences, nlp, ocr, year, pol. If loc, sentences, nlp, ocr, year, pol are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Configuration saved in /datadrive_2/hmd_distilbert-time-reg/config.json\n",
      "Model weights saved in /datadrive_2/hmd_distilbert-time-reg/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/hmd_distilbert-time-reg/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/hmd_distilbert-time-reg/special_tokens_map.json\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-cead509d66e37495.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-a23699cca14a5879.arrow\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12750' max='12750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12750/12750 27:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3383333.632000</td>\n",
       "      <td>3378728.000000</td>\n",
       "      <td>3378728.500000</td>\n",
       "      <td>1838.051636</td>\n",
       "      <td>-11498.947868</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3369933.056000</td>\n",
       "      <td>3366958.000000</td>\n",
       "      <td>3366957.750000</td>\n",
       "      <td>1834.847046</td>\n",
       "      <td>-11458.885796</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3359101.952000</td>\n",
       "      <td>3355413.000000</td>\n",
       "      <td>3355413.000000</td>\n",
       "      <td>1831.698364</td>\n",
       "      <td>-11419.591365</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3346214.912000</td>\n",
       "      <td>3344405.000000</td>\n",
       "      <td>3344405.000000</td>\n",
       "      <td>1828.691162</td>\n",
       "      <td>-11382.123552</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3339139.328000</td>\n",
       "      <td>3334286.250000</td>\n",
       "      <td>3334286.250000</td>\n",
       "      <td>1825.922363</td>\n",
       "      <td>-11347.683780</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3327528.960000</td>\n",
       "      <td>3325418.750000</td>\n",
       "      <td>3325419.000000</td>\n",
       "      <td>1823.492554</td>\n",
       "      <td>-11317.501961</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3320561.920000</td>\n",
       "      <td>3318148.250000</td>\n",
       "      <td>3318148.000000</td>\n",
       "      <td>1821.497803</td>\n",
       "      <td>-11292.755206</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3313964.032000</td>\n",
       "      <td>3312735.750000</td>\n",
       "      <td>3312736.250000</td>\n",
       "      <td>1820.011475</td>\n",
       "      <td>-11274.334216</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3308770.304000</td>\n",
       "      <td>3309396.250000</td>\n",
       "      <td>3309396.250000</td>\n",
       "      <td>1819.093872</td>\n",
       "      <td>-11262.967038</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3307517.952000</td>\n",
       "      <td>3308263.500000</td>\n",
       "      <td>3308263.500000</td>\n",
       "      <td>1818.782471</td>\n",
       "      <td>-11259.111903</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-1000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-1000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-1500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-1500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-2000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-2000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-2500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-2500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-2500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-3000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-3000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-3500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-3500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-4000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-4000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-4500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-4500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-5000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-5000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-5000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-5500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-5500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-6000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-6000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-6500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-6500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-7000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-7000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-7500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-7500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-7500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-8000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-8000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-8500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-8500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-8500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-9000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-9000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-9500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-9500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-10000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-10000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-10000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-10500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-10500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-11000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-11000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-11000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-11500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-11500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-12000\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-12000/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to ../results_bnert-time-y/checkpoint-12500\n",
      "Configuration saved in ../results_bnert-time-y/checkpoint-12500/config.json\n",
      "Model weights saved in ../results_bnert-time-y/checkpoint-12500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1800\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-5d4e3eca5d374f93.arrow\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: loc, sentences, nlp, ocr, year, pol. If loc, sentences, nlp, ocr, year, pol are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: loc, sentences, nlp, ocr, year, pol. If loc, sentences, nlp, ocr, year, pol are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Configuration saved in /datadrive_2/bnert-time-y-time-reg/config.json\n",
      "Model weights saved in /datadrive_2/bnert-time-y-time-reg/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/bnert-time-y-time-reg/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/bnert-time-y-time-reg/special_tokens_map.json\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-b0d6e7333671d38f.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-1b5aeac49d2038d4.arrow\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year. If pol, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-y_masked_25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='12750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  501/12750 00:57 < 23:36, 8.65 it/s, Epoch 0.39/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../results_bnert-time-y_masked_25/checkpoint-500\n",
      "Configuration saved in ../results_bnert-time-y_masked_25/checkpoint-500/config.json\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:319] . unexpected pos 14976 vs 14908",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/torch/serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 379\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/torch/serialization.py:604\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    603\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 604\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     trainer = Trainer(\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     model=model,\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     args=training_args,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     trainer.train()\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m RegressionTrainer(\n\u001b[1;32m     37\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     38\u001b[0m         args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics_for_regression,\n\u001b[1;32m     42\u001b[0m         )\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     test_set \u001b[38;5;241m=\u001b[39m test_set\u001b[38;5;241m.\u001b[39mmap(preprocess_function,fn_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_col\u001b[39m\u001b[38;5;124m'\u001b[39m: sent_col})\n\u001b[1;32m     46\u001b[0m     trainer\u001b[38;5;241m.\u001b[39meval_dataset\u001b[38;5;241m=\u001b[39mtest_set\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/transformers/trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1495\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1497\u001b[0m )\n\u001b[0;32m-> 1498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/transformers/trainer.py:1817\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1819\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/transformers/trainer.py:2042\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2042\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2043\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/transformers/trainer.py:2114\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_flos()\n\u001b[1;32m   2113\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 2114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[1;32m   2116\u001b[0m     \u001b[38;5;66;03m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m     \u001b[38;5;66;03m# config `stage3_gather_16bit_weights_on_model_save` is True\u001b[39;00m\n\u001b[1;32m   2118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/transformers/trainer.py:2602\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2599\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2602\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   2605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/transformers/trainer.py:2654\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   2652\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/transformers/modeling_utils.py:1563\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, **kwargs)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_file, shard \u001b[38;5;129;01min\u001b[39;00m shards\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 1563\u001b[0m     \u001b[43msave_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1566\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel weights saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, WEIGHTS_NAME)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/torch/serialization.py:380\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    379\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m--> 380\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    381\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/torch/serialization.py:259\u001b[0m, in \u001b[0;36m_open_zipfile_writer_buffer.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:319] . unexpected pos 14976 vs 14908"
     ]
    }
   ],
   "source": [
    "result_dict = defaultdict(dict)\n",
    "\n",
    "for name, mdict in model_dict.items():\n",
    "    print(f'Creating a model for {name}')\n",
    "    tokenizer = model_dict[name]['tokenizer']\n",
    "    model = model_dict[name]['model']\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    sent_col = model_dict[name]['sentences']\n",
    "    \n",
    "    #train_val = train_val.map(add_text_col,fn_kwargs={'source': sent_col})\n",
    "    train_val = train_val.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "    seed=1984,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    output_dir=f\"../results_{name}\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.05,\n",
    "        )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_val[\"train\"],\n",
    "#     eval_dataset=train_val[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#         )\n",
    "\n",
    "\n",
    "#     trainer.train()\n",
    "\n",
    "    trainer = RegressionTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_val[\"train\"],\n",
    "        eval_dataset=train_val[\"test\"],\n",
    "        compute_metrics=compute_metrics_for_regression,\n",
    "        )\n",
    "\n",
    "    trainer.train()\n",
    "    test_set = test_set.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    trainer.eval_dataset=test_set\n",
    "    trainer.evaluate()\n",
    "    scores = trainer.evaluate()\n",
    "    #print(scores)\n",
    "    model.save_pretrained(f'/datadrive_2/{name}-time-reg')\n",
    "    tokenizer.save_pretrained(f\"/datadrive_2/{name}-time-reg\")\n",
    "    \n",
    "    \n",
    "    result_dict[name]['loss'] = scores['eval_loss']\n",
    "    result_dict[name]['mae'] = scores['eval_mae']\n",
    "    result_dict[name]['mse'] = scores['eval_mse']\n",
    "    result_dict[name]['accuracy']  = scores['eval_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52123841",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(result_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60d00595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &        loss &       mae &         mse &  accuracy \\\\\n",
      "\\midrule\n",
      "distilbert     &  3318500.25 &  1821.592 &  3318500.25 &       0.0 \\\\\n",
      "hmd\\_distilbert &  3309462.25 &  1819.109 &  3309462.25 &       0.0 \\\\\n",
      "bnert-time-y   &  3307389.25 &  1818.539 &  3307389.50 &       0.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531548/2842510611.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(results_df.round(3).to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(results_df.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebcd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('tables/classsify_time_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b78a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_df = pd.read_csv('tables/classsify_time_1.csv')\n",
    "print(results_df.round(3).to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d29c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8bc70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm2",
   "language": "python",
   "name": "lm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
