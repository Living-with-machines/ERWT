{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import DataCollatorWithPadding\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b07302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_from_disk('/datadrive_2/')\n",
    "#test_data = dataset['test']\n",
    "\n",
    "cache_dir = '/datadrive_2/hf_cache/'\n",
    "dataset = load_from_disk(\"/datadrive_2/HMD_chunked_100_test/\")\n",
    "dataset #= dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a047ed",
   "metadata": {},
   "source": [
    "##Â Classify by Political Leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_pattern = re.compile(r'\\bliberal|\\bconservat|\\btory\\b|\\btories\\b',re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69dea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_pattern.findall('liberal governments do not fire their conservative political ministers minister')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sent_split(x):\n",
    "#      return {'data': [\n",
    "#                 {'sentence':s.lower(),\n",
    "#                  'length': len(s.split()),\n",
    "#                  'pol': p, 'loc':l, 'year':y, 'ocr':o,'nlp':n} \n",
    "#                      for y,p,l,o,n,t in zip(x['year'],x['pol'],x['loc'],x['ocr_quality_mean'],x['nlp'],x['text']) \n",
    "#                       for s in sent_tokenize(t) \n",
    "#                          if pol_pattern.findall(s)\n",
    "#                  ]\n",
    "#             }\n",
    "\n",
    "# test_data = dataset.map(sent_split,batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db872537",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset.map(lambda x: {'sentences': x['sentences'].lower()}, num_proc=6).shuffle(seed=42).select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0086b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3edfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = test_data.filter(lambda x: x['data.length'] > 25).shuffle(seed=42).select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pred_data(example,add_field='year'):\n",
    "#     return {'st_year_sep': f'[{example[add_field]}]' + ' [SEP] ' + example['sentences'] ,\n",
    "#      'year_sep': str(example[add_field]) + ' [SEP] ' + example['sentences'] ,\n",
    "#      'year_date': str(example[add_field]) + ' [DATE] ' + example['sentences'],\n",
    "        \n",
    "#     }\n",
    "def pred_data(example):\n",
    "    return {'label':float(example['year'])}\n",
    "    \n",
    "data = test_data.map(pred_data , num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad873104",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3c018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a32db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab2code = {'[con]':0,'[lib]':1,'[rad]':2,'[neutr]':3,'[none]':4}\n",
    "# num_labels = len(lab2code)\n",
    "# data = data.map(lambda x: {'label': lab2code[x['pol']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcecd756",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(len(data)*.2)\n",
    "train_test = data.train_test_split(test_size=test_size, seed=1984)\n",
    "test_set = train_test['test']\n",
    "val_size = int(len(train_test['train'])*.15)\n",
    "train_val =  train_test['train'].train_test_split(test_size=val_size, seed=1984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f7f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [('distilbert','distilbert-base-uncased','[SEP]','sentences'),\n",
    "               ('hmd_distilbert','/datadrive_2/bnert-hmd','[SEP]','sentences'),\n",
    "               #('bnert-time-st-y','/datadrive_2/bnert-time-st-y','[SEP]','sentences'),\n",
    "               ('bnert-time-y','/datadrive_2/bnert-time-y','[DATE]','sentences'),\n",
    "               ('bnert-time-y_masked_25','/datadrive_2/bnert-time-y_masked_25','[DATE]','sentences'),\n",
    "               ('bnert-time-y_masked_75','/datadrive_2/bnert-time-y_masked_75','[DATE]','sentences'),\n",
    "               #('bnert-pol-st','/datadrive_2/bnert-pol-st','[SEP]','year_sep'),\n",
    "               #('bnert-pol','/datadrive_2/bnert-pol','[SEP]','sentences')\n",
    "              ]\n",
    "\n",
    "model_dict = defaultdict(dict)\n",
    "for name,checkpoint, st, sent_col in checkpoints:\n",
    "    model_dict[name]['model'] = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=1)\n",
    "    model_dict[name]['tokenizer'] = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    #model_dict[name]['special_token'] = st\n",
    "    model_dict[name]['sentences'] = sent_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f12775",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = train_val.remove_columns(['nlp', 'ocr', 'loc'])\n",
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fed10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def add_text_col(example,source):\n",
    "#    return {'text' : example[source]}\n",
    "\n",
    "def preprocess_function(examples, target_col):\n",
    "    return tokenizer(examples[target_col], truncation=True, padding=\"max_length\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    #print(labels, logits)\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "    single_squared_errors = ((logits - labels).flatten()**2).tolist()\n",
    "    \n",
    "    # Compute accuracy \n",
    "    # Based on the fact that the rounded score = true score only if |single_squared_errors| < 0.5\n",
    "    accuracy = sum([1 for e in single_squared_errors if e < 0.25]) / len(single_squared_errors)\n",
    "    \n",
    "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2, \"accuracy\": accuracy}\n",
    "\n",
    "class RegressionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0][:, 0]\n",
    "        loss = torch.nn.functional.mse_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e895a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(dict)\n",
    "\n",
    "for name, mdict in model_dict.items():\n",
    "    print(f'Creating a model for {name}')\n",
    "    tokenizer = model_dict[name]['tokenizer']\n",
    "    model = model_dict[name]['model']\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    sent_col = model_dict[name]['sentences']\n",
    "    \n",
    "    #train_val = train_val.map(add_text_col,fn_kwargs={'source': sent_col})\n",
    "    train_val = train_val.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "    seed=1984,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    output_dir=f\"./results_{name}\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_val[\"train\"],\n",
    "#     eval_dataset=train_val[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#         )\n",
    "\n",
    "\n",
    "#     trainer.train()\n",
    "\n",
    "    trainer = RegressionTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_val[\"train\"],\n",
    "        eval_dataset=train_val[\"test\"],\n",
    "        compute_metrics=compute_metrics_for_regression,\n",
    "        )\n",
    "\n",
    "    trainer.train()\n",
    "    test_set = test_set.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    trainer.eval_dataset=test_set\n",
    "    trainer.evaluate()\n",
    "    scores = trainer.evaluate()\n",
    "    #print(scores)\n",
    "    model.save_pretrained(f'/datadrive_2/{name}-pol')\n",
    "    tokenizer.save_pretrained(f\"/datadrive_2/{name}-pol\")\n",
    "    \n",
    "    \n",
    "    #predictions = trainer.predict(test_set)\n",
    "    #preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    result_dict[name]['mae'] = scores['eval_mae']#f1_score(preds,predictions.label_ids,average='macro')\n",
    "    result_dict[name]['mse'] = scores['eval_mse']#f1_score(preds,predictions.label_ids,average='micro')\n",
    "    result_dict[name]['accuracy']  = scores['eval_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52123841",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(result_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d00595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebcd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('tables/classsify_time.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b78a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm2",
   "language": "python",
   "name": "lm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
