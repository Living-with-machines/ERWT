{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c47094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset,load_from_disk\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec61b71",
   "metadata": {},
   "source": [
    "We set a directory for caching the dataset files, otherwise the `/home` directory will overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d75097",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '/datadrive_2/hf_cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87938390",
   "metadata": {},
   "source": [
    "## Load Dataset and filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f7b9f",
   "metadata": {},
   "source": [
    "Import the datasets from the HuggingFace hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"davanstrien/hmd_newspapers\", cache_dir=cache_dir)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b505b",
   "metadata": {},
   "source": [
    "Because some files seem to lack a date, we filter first to avoid errors later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: (x['date'] is not None) \\\n",
    "                         or (x['ocr_quality_mean'] is not None), num_proc=6)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c87116",
   "metadata": {},
   "source": [
    "## Link Data\n",
    "\n",
    "In this section of the notebook we focus on further enriching an contextualizing the data by adding the political leaning as a variable. We also add the NLP identifier (used by FMP) which can be used later for adding metadata.\n",
    "\n",
    "We will map the political leaning to special tokens added to the tokenizer instance below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from NLP to political leaning\n",
    "# this is created semi-manually for this limited dataset\n",
    "nlp2pol = {2083:'neutral',\n",
    " 2084:'neutral',\n",
    " 2085:'neutral',\n",
    " 2088:'conservative',\n",
    " 2089:'conservative',\n",
    " 2090:'conservative',\n",
    " 2194:'liberal',\n",
    " 2244:'none',\n",
    " 2642:'liberal',\n",
    " 2643:'conservative', # not found\n",
    " 2644:'conservative',\n",
    " 2645:'conservative',\n",
    " 2646:'none', # https://en.wikipedia.org/wiki/The_Star_(1788)\n",
    " 2647:'radical', # https://www.britishnewspaperarchive.co.uk/titles/statesman-london\n",
    "}\n",
    "\n",
    "pol2code = {'none':'[none]','neutral':'[neutr]','conservative':'[con]','liberal':'[lib]','radical':'[rad]'}\n",
    "\n",
    "loc2code = {'Liverpool, Merseyside, England':'[liverpool]', 'London, England':'[london]'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a52f58c",
   "metadata": {},
   "source": [
    "Because the original data lacks an NLP identifier for now, we add it via the title. For this we first need to create a mapping between NLP and titles for each publication year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import defaultdict\n",
    "nlp_dict = defaultdict(dict)\n",
    "url = 'https://raw.githubusercontent.com/Living-with-machines/hmd_url_generator/main/HMD_title_urls.json'\n",
    "data = requests.get(url).json()\n",
    "for newspaper, year_dict in data.items():\n",
    "    for year, info_dict in year_dict.items():\n",
    "        if info_dict['fname'].endswith('zip'):\n",
    "            nlp_dict[newspaper][int(year)] = int(info_dict['fname'].split('_')[1])\n",
    "nlp_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4eb046",
   "metadata": {},
   "source": [
    "To match the titles between the `dataset` and the `nlp_dict` we need just one more dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtitle2title = {'The Northern Daily Times.':'The Northern Daily Times etc',\n",
    " 'Northern Times.':'The Northern Daily Times etc',\n",
    " 'The Daily Times.':'The Northern Daily Times etc',\n",
    " 'The Liverpool Standard and General Commercial Advertiser.':'The Liverpool Standard etc',\n",
    " 'The Liverpool Standard, and General Advertiser.':'The Liverpool Standard etc',\n",
    " 'The Sun.':'The Sun',\n",
    " 'Colored News.':'Colored News',\n",
    " 'The Express.':'The Express',\n",
    " 'The National Register.':'National Register.',\n",
    " 'The Press.':'The Press.',\n",
    " 'Star.':'The Star',\n",
    " 'The Statesman.':'The Statesman'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02c2dd",
   "metadata": {},
   "source": [
    "## Prepare Chunked Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a57c1c",
   "metadata": {},
   "source": [
    "The code below extends information on each article by\n",
    "- splitting the date into year and month\n",
    "- add an NLP identifier as well codes for location and political leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d18f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cols(example):\n",
    "\n",
    "    nlp = nlp_dict[dtitle2title[example['title']]][example['date'].year]\n",
    "    \n",
    "    return {'year': example['date'].year,\n",
    "            'length': len(example['text'].split()),\n",
    "            'month': example['date'].month,\n",
    "            'nlp' : nlp,\n",
    "            'pol': pol2code[nlp2pol[nlp]],\n",
    "            'loc': loc2code[example['location']]\n",
    "           }\n",
    "\n",
    "\n",
    "dataset = dataset.map(add_cols , num_proc=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03556a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk('/datadrive_2/HMD_context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53147bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = pd.Series(dataset['train']['length'])\n",
    "lengths.plot(kind='hist',bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b08bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lengths > 100) / len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49605e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_long = dataset.filter(lambda x: x['length'] >= 100, num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset['train']),len(dataset_long['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19175da3",
   "metadata": {},
   "source": [
    "After replacing the columns with codes we can remove some information from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b9b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_long = dataset_long['train'].remove_columns(['title', 'location', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_small = dataset.shuffle(seed=0).select(range(100_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8259f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include_cols = []\n",
    "\n",
    "# def sent_split(x):\n",
    "#      return {'data': [\n",
    "#                 {'sentence':s.lower(),\n",
    "#                  'length': len(s.split()),\n",
    "#                  'pol': p, 'loc':l, 'year':y, 'ocr':o,'nlp':n} \n",
    "#                      for y,p,l,o,n,t in zip(x['year'],x['pol'],x['loc'],x['ocr_quality_mean'],x['nlp'],x['text']) \n",
    "#                       for s in sent_tokenize(t)]\n",
    "            \n",
    "            \n",
    "#             }\n",
    "     \n",
    "def batched_prepare(examples, chunk_size=100):\n",
    "    sentList, polList, locList, ocrList, yearList, nlpList = [],[],[],[],[],[]\n",
    "    \n",
    "    for text, pol, loc, ocr, year, nlp in zip(examples['text'],\n",
    "                                         examples['pol'],\n",
    "                                         examples['loc'],\n",
    "                                         examples['ocr_quality_mean'],\n",
    "                                         examples['year'],\n",
    "                                         examples['nlp']):\n",
    "        text = text.split()\n",
    "        chunks = [' '.join(text[i:i+chunk_size]) for i in range(0,len(text),chunk_size)]\n",
    "        \n",
    "        sentList.extend(chunks)\n",
    "        #lengthList.extend([len(s.split()) for s in sentences])\n",
    "        polList.extend([pol]*len(chunks))\n",
    "        locList.extend([loc]*len(chunks))\n",
    "        ocrList.extend([ocr]*len(chunks))\n",
    "        yearList.extend([year]*len(chunks))\n",
    "        nlpList.extend([nlp]*len(chunks))\n",
    "        \n",
    "    return {\"sentences\": sentList, \"loc\": locList, \"pol\": polList,\n",
    "             \"ocr\": ocrList, \"year\": yearList, 'nlp': nlpList, \n",
    "           }\n",
    "\n",
    "            \n",
    "chunked_corpus = dataset_long.map(batched_prepare, \n",
    "                              batched=True, \n",
    "                              num_proc=12,\n",
    "                              remove_columns=dataset_long.column_names\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_corpus[30002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d96edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunked_corpus),len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_corpus_flat = sent_corpus.flatten()\n",
    "# sent_corpus_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_corpus_split = chunked_corpus.train_test_split(\n",
    "                        test_size=.1, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_corpus_split.save_to_disk('/datadrive_2/HMD_chunked_100_context')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0414cb0",
   "metadata": {},
   "source": [
    "### End of Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e0be7",
   "metadata": {},
   "source": [
    "## Add special tokens to tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_corpus_split = load_from_disk('/datadrive_2/HMD_chunked_100_context'); chunked_corpus_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #special year tokens\n",
    "# added_tokens = []\n",
    "# for mf in ['pol','loc']:\n",
    "#     added_tokens.extend(chunked_corpus_split['train'].unique(mf))\n",
    "    \n",
    "        \n",
    "# added_tokens.extend([f\"[{e}]\" for e in chunked_corpus_split['train'].unique('year')])\n",
    "# tokenizer.add_tokens(added_tokens, )\n",
    "\n",
    "# special_tokens =['[MET]','[DATE]','[POL]','[LOC]']\n",
    "# metadata_tokens = {f'additional_special_tokens': special_tokens}\n",
    "# tokenizer.add_special_tokens(metadata_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4adc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no special year tokens\n",
    "special_tokens =['[DATE]','[POL]','[LOC]']\n",
    "metadata_tokens = {f'additional_special_tokens': special_tokens}\n",
    "tokenizer.add_special_tokens(metadata_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"distilbert-hmd-uncased-combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_corpus_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6616c1c",
   "metadata": {},
   "source": [
    "## Further data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc17a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = chunked_corpus_split['train'].train_test_split(test_size=.65, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_val_split['test'].save_to_disk('/datadrive_2/HMD_chunked_100_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbc844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_val_split = train_val_split['train'].train_test_split(test_size=.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80fbf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa346cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = train_val_split.map(lambda example: {'length': [len(x.split()) for x in example['sentences']]}, \n",
    "                                        batched=True, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc47cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = train_val_split.filter(lambda x: x['length'] > 50, num_proc=6); train_val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c482c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train_val_split['train']['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19fbf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split.save_to_disk('/datadrive_2/frozen_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479aa661",
   "metadata": {},
   "source": [
    "# Change Preprocessing/Training From Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = load_from_disk('/datadrive_2/frozen_corpus')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-hmd-uncased-combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f97430",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_year_as_special_token(examples):\n",
    "    return {'sent_context': [f'[{y}] [SEP] {s}' for y,s in zip(examples['year'],examples['sentences'])]}\n",
    "\n",
    "#def prepend_pol_as_special_token(examples):\n",
    "#    return {'sent_context': [f'{p} [SEP] {s}' for p,s in zip(examples['pol'],examples['sentences'])]}\n",
    "\n",
    "def prepend_year(examples):\n",
    "    return {'sent_context': [f'{y} [DATE] {s}' for y,s in zip(examples['year'],examples['sentences'])]}\n",
    "\n",
    "def no_prepend(examples):\n",
    "    return {'sent_context': [s for s in examples['sentences']]}\n",
    "\n",
    "code2pol = {'[none]':'none','[neutr]':'neutral','[con]':'conservative','[lib]':'liberal','[rad]':'radical'}\n",
    "code2loc = {'[london]':'london','[liverpool]':'liverpool'}\n",
    "\n",
    "def prepend_combined(examples):\n",
    "    return {'sent_context': [f'{y} [DATE] {code2pol[p]} [POL] {code2loc[l]} [LOC] {s}' for y,p,l,s in zip(examples['year'],\n",
    "                                                              examples['pol'],\n",
    "                                                              examples['loc'],\n",
    "                                                              examples['sentences'])]}\n",
    "\n",
    "\n",
    "training_corpus = train_val_split.map(prepend_combined, batched=True, num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c93c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(training_corpus['train'][10]['sent_context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sents(examples):\n",
    "    return tokenizer(examples['sent_context'],\n",
    "             truncation=True,\n",
    "             padding='max_length',\n",
    "             max_length=256,\n",
    "             return_overflowing_tokens=False)\n",
    "    \n",
    "\n",
    "tokenized_contextualized = training_corpus.map(tokenize_sents,\n",
    "                                                   batched=True,\n",
    "                                                   num_proc=12,\n",
    "                                                   remove_columns=training_corpus['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_contextualized['train'][10]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729dd734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_contextualized = tokenized_contextualized.map(\n",
    "                lambda x: {'labels': [y.copy() for y in x['input_ids']]},\n",
    "                batched=True,\n",
    "                num_proc=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68572015",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_contextualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b067a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_contextualized.save_to_disk('/datadrive_2/frozen_backup')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4456b7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca73546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_contextualized = load_from_disk('/datadrive_2/frozen_backup')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-hmd-uncased-combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7516f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5234550\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 581857\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_contextualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a37fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a686b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "def masking_metadata_collator(features):\n",
    "    #print(features)\n",
    "    for feature in features:\n",
    "        \n",
    "        input_ids = np.array(feature[\"input_ids\"])\n",
    "        no_pad = input_ids[np.where(input_ids > 0)[0]]\n",
    "        labels = np.array(feature[\"labels\"])\n",
    "        new_labels = np.array([-100]*len(no_pad))\n",
    "        mask = np.random.binomial(1, .15, (len(no_pad),))\n",
    "        mask[1] = np.random.binomial(1, .75, size=1)\n",
    "        idxs = np.where(mask)\n",
    "        input_ids[idxs] = tokenizer.mask_token_id\n",
    "        \n",
    "        feature['input_ids'] = input_ids\n",
    "        \n",
    "    return default_data_collator(features)\n",
    "\n",
    "def masking_combined_metadata_collator(features):\n",
    "    #print(features)\n",
    "    for feature in features:\n",
    "        \n",
    "        input_ids = np.array(feature[\"input_ids\"])\n",
    "        no_pad = input_ids[np.where(input_ids > 0)[0]]\n",
    "        labels = np.array(feature[\"labels\"])\n",
    "        new_labels = np.array([-100]*len(no_pad))\n",
    "        mask = np.random.binomial(1, .15, (len(no_pad),))\n",
    "        mask[1] = np.random.binomial(1, .25, size=1)\n",
    "        mask[3] = np.random.binomial(1, .25, size=1)\n",
    "        mask[5] = np.random.binomial(1, .25, size=1)\n",
    "        idxs = np.where(mask)\n",
    "        input_ids[idxs] = tokenizer.mask_token_id\n",
    "        \n",
    "        feature['input_ids'] = input_ids\n",
    "        \n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d71fddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 1850 [DATE] liberal [POL] london [LOC] of unusual importance, that itaffected in a great degree the future peace of thisem - pire, and he bored it would receive from the! lousetheconsideration it deserved. sir george grey was not in the least disposedto treat with indifference the question b fore the! louse ; nor did he think the house would ho indifferentto any practical measure on the subject. — ( hear, hear. ) the bill now before the house contemplatedthe extension of the summary jurisdiction possessed bymagistrates with regard to juvenile offenders. thatwas nothing more nor less then to leave the jaw pre - cisely es it was under the juvenile offendersact, introduced by his honourable friend themember ler [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_contextualized['train'][1000]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94235697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30525, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12d5c102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81789\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_contextualized[\"train\"]) // batch_size\n",
    "print(logging_steps)\n",
    "model_name = model_checkpoint.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c7fa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert-base-uncased'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "709c656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    dataloader_drop_last=True,\n",
    "    output_dir=f\"/datadrive_2/{model_name}-combined-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=10000,\n",
    "    #logging_steps=2000,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bb13ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_contextualized[\"train\"],\n",
    "    eval_dataset=tokenized_contextualized[\"test\"],\n",
    "    data_collator=masking_combined_metadata_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "018c4873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5234550\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81789\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/data/data_collator.py:131: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  batch[k] = torch.tensor([f[k] for f in features])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81789' max='81789' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81789/81789 25:45:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.341500</td>\n",
       "      <td>0.313347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 581857\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=81789, training_loss=0.34152094108009634, metrics={'train_runtime': 92757.5027, 'train_samples_per_second': 56.433, 'train_steps_per_second': 0.882, 'total_flos': 3.469454447905014e+17, 'train_loss': 0.34152094108009634, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c82dd719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /datadrive_2/bnert-combined/config.json\n",
      "Model weights saved in /datadrive_2/bnert-combined/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/bnert-combined/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/bnert-combined/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/datadrive_2/bnert-combined/tokenizer_config.json',\n",
       " '/datadrive_2/bnert-combined/special_tokens_map.json',\n",
       " '/datadrive_2/bnert-combined/vocab.txt',\n",
       " '/datadrive_2/bnert-combined/added_tokens.json',\n",
       " '/datadrive_2/bnert-combined/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('/datadrive_2/bnert-combined')\n",
    "tokenizer.save_pretrained(\"/datadrive_2/bnert-combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a9d1396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /datadrive_2/bnert-combined_backup/config.json\n",
      "Model weights saved in /datadrive_2/bnert-combined_backup/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/bnert-combined_backup/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/bnert-combined_backup/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/datadrive_2/bnert-combined_backup/tokenizer_config.json',\n",
       " '/datadrive_2/bnert-combined_backup/special_tokens_map.json',\n",
       " '/datadrive_2/bnert-combined_backup/vocab.txt',\n",
       " '/datadrive_2/bnert-combined_backup/added_tokens.json',\n",
       " '/datadrive_2/bnert-combined_backup/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('/datadrive_2/bnert-combined_backup')\n",
    "tokenizer.save_pretrained(\"/datadrive_2/bnert-combined_backup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc1eea",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b682600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #samples= [tokenized_contextualized_small[:3]]\n",
    "# samples= [tokenized_contextualized_split['train'][i] for i in range(10)]\n",
    "# batch = data_collator(samples)\n",
    "\n",
    "# for chunk in batch[\"input_ids\"]:\n",
    "#     print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012163e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('/datadrive_2/bnert-time-y-backup')\n",
    "tokenizer.save_pretrained(\"/datadrive_2/bnert-time-y-backup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33546a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.save_pretrained(\"/datadrive_2/bnert_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbff9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = AutoModelForMaskedLM.from_pretrained('bnert_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ed7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=\"/datadrive_2/test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filler(\"[CLS] [MASK] [SEP] confident it will meet with that warm and generalsupport which its just and sound policy alike demand. by thus according to the people of ireland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d88ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_contextualized['train'][200000]['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1820 [SEP] 2 [SEP] 0 [SEP] the Prime Minister is Mr. [MASK].'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1820 [SEP] 3 [SEP] 0 [SEP] there is plenty of [MASK] machines.'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")\n",
    "#preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81228aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1820 [SEP] 6 [SEP] 14 [SEP] He was involved in a [MASK] accident.'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd291cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "'1820 6 14 he was involved in a serious accident.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1810 [SEP] 2 [SEP] 1 [SEP] [MASK] Majesty.'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '[MASK] [SEP] 2 [SEP] 1 [SEP] The war between Denmark and Germany took a deadly toll.'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '[MASK] [SEP] 6 [SEP] 14 [SEP] The war between France and Germany took a deadly toll.'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b525126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1880 [SEP] 6 [SEP] 14 [SEP] The war in [MASK] took a deadly toll.'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1880 [SEP] 2 [SEP] 14 [SEP] The revolution in [MASK].'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf924fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1880 [SEP] [MASK] [SEP] 1 [SEP] liberal progress opinion liberal progress opinion liberal progress opinion.'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d3eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea48a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1870 [SEP] 0 [SEP] 0 [SEP] The train is leaving in [MASK].'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc8b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol2id, loc2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1830 [SEP] 0 [SEP] 1 [SEP] The train is heading towards [MASK].'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence'].upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d482a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1830 [SEP] 0 [SEP] [MASK] [SEP] This paper is published in Manchester.'\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence'].upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f8c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('1830 [SEP] 0 [SEP] 1 [SEP] The train is heading towards [MASK].', return_tensors=\"pt\")\n",
    "outputs = model(**inputs.to('cuda'))\n",
    "\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a333d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmodel = AutoModelForTokenClassification.from_pretrained('bnert_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a74db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmodel.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59d10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('1830 [SEP] 0 [SEP] 1 [SEP] The train is heading towards [MASK].', return_tensors=\"pt\")\n",
    "outputs = tmodel(**inputs.to('cuda'))\n",
    "\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fpipe = pipeline('feature-extraction',model='bnert_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe45119",
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = fpipe('1880 [SEP] 0 [SEP] 1 [SEP] The train is heading towards london.')\n",
    "pen1 = features1[0][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a460583",
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = fpipe('1820 [SEP] 0 [SEP] 0 [SEP] The train is heading towards london.')\n",
    "pen2 = features2[0][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "cosine(pen1,pen2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb4cdd",
   "metadata": {},
   "source": [
    "## To revisit later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd63bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "def masking_metadata_collator(features):\n",
    "    #print(features)\n",
    "    for feature in features:\n",
    "        \n",
    "        input_ids = np.array(feature[\"input_ids\"])\n",
    "        no_pad = input_ids[np.where(input_ids > 0)[0]]\n",
    "        labels = np.array(feature[\"labels\"])\n",
    "        new_labels = np.array([-100]*len(no_pad))\n",
    "        mask = np.random.binomial(1, .15, (len(no_pad),))\n",
    "        mask[1] = np.random.binomial(1, .75, size=1)\n",
    "        idxs = np.where(mask)\n",
    "        input_ids[idxs] = tokenizer.mask_token_id\n",
    "        \n",
    "        feature['input_ids'] = input_ids\n",
    "        \n",
    "    return default_data_collator(features)\n",
    "\n",
    "\n",
    "#samples= [tokenized_contextualized_small[:3]]\n",
    "samples= [tokenized_contextualized['train'][i] for i in range(10)]\n",
    "batch = masking_sentence(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8996ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm2",
   "language": "python",
   "name": "lm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
