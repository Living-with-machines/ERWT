{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8c000e",
   "metadata": {},
   "source": [
    "Code adapted from this [Stack Overflow](\n",
    "https://stackoverflow.com/questions/70464428/how-to-calculate-perplexity-of-a-sentence-using-huggingface-masked-language-mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7dfd606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "80638367",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/datadrive_2/frozen_corpus')\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5b5de0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4733cf9c6e4d37afa7b8c5486d4750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/17 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d38ee4b2d104794859191646000c9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/17 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb7a562fbf1438ab0e95d30682e7791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/17 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08d674455a849d5929c94425f618741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/16 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9957ebffa740a887a94b0439f89865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/17 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a9b72b21264ce48abdfc828e26dd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/16 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pred_data(example):\n",
    "    return {'st_year_sep': f'[{example[\"year\"]}]' + ' [SEP] ' + example['sentences'] ,\n",
    "     'year_sep': str(example['year']) + ' [SEP] ' + example['sentences'] ,\n",
    "     'year_date': str(example['year']) + ' [DATE] ' + example['sentences'] \n",
    "        \n",
    "    }\n",
    "    \n",
    "test_data = test_data.map(pred_data , num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "12441e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "db2e5c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr', 'length', 'st_year_sep', 'year_sep', 'year_date'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4082ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [('distilbert','distilbert-base-uncased','[SEP]','year_sep'),\n",
    "               ('hmd_distilbert','/datadrive_2/bnert-hmd','[SEP]','year_sep'),\n",
    "               ('bnert-time-st-y','/datadrive_2/bnert-time-st-y','[SEP]','st_year_sep'),\n",
    "               ('bnert-time-y','/datadrive_2/bnert-time-y','[DATE]','year_date'),\n",
    "               ('bnert-time-y_masked_25','/datadrive_2/bnert-time-y_masked_25','[DATE]','year_date'),\n",
    "               ('bnert-time-y_masked_75','/datadrive_2/bnert-time-y_masked_75','[DATE]','year_date')]\n",
    "\n",
    "model_dict = defaultdict(dict)\n",
    "for name,checkpoint, st, sent_col in checkpoints:\n",
    "    model_dict[name]['model'] = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "    model_dict[name]['tokenizer'] = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    #model_dict[name]['special_token'] = st\n",
    "    model_dict[name]['sentences'] = sent_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2637f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_perplexity(example, sent_col, name, model, tokenizer):\n",
    "    tensor_input = tokenizer.encode(example[sent_col], return_tensors='pt',truncation=True, max_length=128)\n",
    "    #print(tensor_input.shape)\n",
    "    #if with_meta:\n",
    "    repeat_input = tensor_input.repeat(tensor_input.size(-1)-4, 1)\n",
    "    mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[2:-2]\n",
    "    #else:\n",
    "    #    repeat_input = tensor_input.repeat(tensor_input.size(-1)-2, 1)\n",
    "    #    mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2]\n",
    "    masked_input = repeat_input.masked_fill(mask == 1, tokenizer.mask_token_id)\n",
    "    labels = repeat_input.masked_fill( masked_input != tokenizer.mask_token_id, -100)\n",
    "    with torch.inference_mode():\n",
    "        loss = model(masked_input, labels=labels).loss\n",
    "    return {f'loss_{name}':np.exp(loss.item())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaec880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert\n",
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8a3ba4fd2644c38aeaefb1579edbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/34 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5bd60de2cf46fabd64ebacfdd2f871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/33 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042f9cf633f3450880ab5367bd37816b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/33 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name, ndict in model_dict.items():\n",
    "    print(f'Evaluating {name}')\n",
    "    test_data = test_data.map(pseudo_perplexity, \n",
    "                              num_proc=3,\n",
    "                              fn_kwargs={'sent_col':ndict['sentences'],\n",
    "                                        'name': name,\n",
    "                                        'model':ndict['model'],\n",
    "                                        'tokenizer':ndict['tokenizer']  \n",
    "                                   }\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "702c7649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.350935507707432"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_pseudo_perplexity(model,tokenizer,'1845 [SEP] London is the capital of Great Britain.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9cfb466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  9512, 30522,   103,  2003,  1996,  3007,  1997,  2307,  3725,\n",
      "          1012,   102],\n",
      "        [  101,  9512, 30522,  2414,   103,  1996,  3007,  1997,  2307,  3725,\n",
      "          1012,   102],\n",
      "        [  101,  9512, 30522,  2414,  2003,   103,  3007,  1997,  2307,  3725,\n",
      "          1012,   102],\n",
      "        [  101,  9512, 30522,  2414,  2003,  1996,   103,  1997,  2307,  3725,\n",
      "          1012,   102],\n",
      "        [  101,  9512, 30522,  2414,  2003,  1996,  3007,   103,  2307,  3725,\n",
      "          1012,   102],\n",
      "        [  101,  9512, 30522,  2414,  2003,  1996,  3007,  1997,   103,  3725,\n",
      "          1012,   102],\n",
      "        [  101,  9512, 30522,  2414,  2003,  1996,  3007,  1997,  2307,   103,\n",
      "          1012,   102],\n",
      "        [  101,  9512, 30522,  2414,  2003,  1996,  3007,  1997,  2307,  3725,\n",
      "           103,   102]])\n",
      "tensor([[-100, -100, -100, 2414, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, 2003, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, 1996, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, 3007, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, 1997, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 2307, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, 3725, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1012, -100]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.744492316460395"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_pseudo_perplexity(model,tokenizer,'1845 [DATE] London is the capital of Great Britain.',with_meta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "02bbd64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_random_mask(model, tokenizer, sentence,meta_pos=None):\n",
    "    tensor_input = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    #print(tensor_input)\n",
    "    repeat_input = torch.clone(tensor_input)\n",
    "    #print(repeat_input)\n",
    "    sum_mask,i = 0,0\n",
    "    while sum_mask == 0:\n",
    "        mask = torch.tensor(np.random.binomial(1, .15, repeat_input.shape[1]))\n",
    "        sum_mask = sum(mask)\n",
    "        \n",
    "    if meta_pos:\n",
    "        mask[meta_pos] = 0\n",
    "    masked_input = repeat_input.masked_fill(mask == 1, tokenizer.mask_token_id)\n",
    "    print(masked_input)\n",
    "    labels = repeat_input.masked_fill( masked_input != tokenizer.mask_token_id, -100)\n",
    "    print(labels)\n",
    "    with torch.inference_mode():\n",
    "        loss = model(masked_input, labels=labels).loss\n",
    "    return np.exp(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fce7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd0902",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm2",
   "language": "python",
   "name": "lm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
