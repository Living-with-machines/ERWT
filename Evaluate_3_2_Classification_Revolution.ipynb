{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560b8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import DataCollatorWithPadding\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "702f2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv('../animacy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5945313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-36ee4eefb063a0df\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/kbeelen/.cache/huggingface/datasets/csv/default-36ee4eefb063a0df/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfc54436d974f0abeb835a8987252c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c6a66733bd4d099621e326c3249cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/kbeelen/.cache/huggingface/datasets/csv/default-36ee4eefb063a0df/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1253becf864af48ae76c8a6217aeaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"../revolution.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a7716ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"{'date': -3661459200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9283, 'title': 'The Northern Daily Times.', 'score': 0.4916961789}\",\n",
       " \"{'date': -3661286400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9762000000000001, 'title': 'The Northern Daily Times.', 'score': 0.4904969335}\",\n",
       " \"{'date': -3666988800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9552, 'title': 'The Northern Daily Times.', 'score': 0.48738962410000003}\",\n",
       " \"{'date': -3663792000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.888, 'title': 'The Northern Daily Times.', 'score': 0.513350904}\",\n",
       " \"{'date': -3662496000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9436, 'title': 'The Northern Daily Times.', 'score': 0.48617035150000004}\",\n",
       " \"{'date': -3668630400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9444, 'title': 'The Northern Daily Times.', 'score': 0.5212857127}\",\n",
       " \"{'date': -3664915200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9601000000000001, 'title': 'The Northern Daily Times.', 'score': 0.4786927998}\",\n",
       " \"{'date': -3665606400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9749000000000001, 'title': 'The Northern Daily Times.', 'score': 0.47370579840000004}\",\n",
       " \"{'date': -3667939200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9146000000000001, 'title': 'The Northern Daily Times.', 'score': 0.4680972993}\",\n",
       " \"{'date': -3668198400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9269000000000001, 'title': 'The Northern Daily Times.', 'score': 0.5328446627}\",\n",
       " \"{'date': -3666384000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8748, 'title': 'The Northern Daily Times.', 'score': 0.4654217064}\",\n",
       " \"{'date': -3665174400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9811000000000001, 'title': 'The Northern Daily Times.', 'score': 0.5369511247000001}\",\n",
       " \"{'date': -3662323200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9486, 'title': 'The Northern Daily Times.', 'score': 0.46061813830000004}\",\n",
       " \"{'date': -3662755200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9151, 'title': 'The Northern Daily Times.', 'score': 0.5433472395}\",\n",
       " \"{'date': -3597264000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8496, 'title': 'The Northern Daily Times.', 'score': 0.462782383}\",\n",
       " \"{'date': -3597177600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.845, 'title': 'The Northern Daily Times.', 'score': 0.4683600664}\",\n",
       " \"{'date': -3596400000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.787, 'title': 'The Northern Daily Times.', 'score': 0.41298168900000004}\",\n",
       " \"{'date': -3596313600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9279000000000001, 'title': 'The Northern Daily Times.', 'score': 0.3358841836}\",\n",
       " \"{'date': -3596140800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9723, 'title': 'The Northern Daily Times.', 'score': 0.3536662459}\",\n",
       " \"{'date': -3594585600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9605, 'title': 'The Northern Daily Times.', 'score': 0.3589557409}\",\n",
       " \"{'date': -3593980800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9045000000000001, 'title': 'The Northern Daily Times.', 'score': 0.4351866841}\",\n",
       " \"{'date': -3593894400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9355, 'title': 'The Northern Daily Times.', 'score': 0.4338502884}\",\n",
       " \"{'date': -3593894400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9679000000000001, 'title': 'The Northern Daily Times.', 'score': 0.5375754833}\",\n",
       " \"{'date': -3593376000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8526, 'title': 'The Northern Daily Times.', 'score': 0.4388594031}\",\n",
       " \"{'date': -3593030400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8899, 'title': 'The Northern Daily Times.', 'score': 0.5280666351000001}\",\n",
       " \"{'date': -3592166400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.7579, 'title': 'The Northern Daily Times.', 'score': 0.582439959}\",\n",
       " \"{'date': -3592166400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9612, 'title': 'The Northern Daily Times.', 'score': 0.6076440215000001}\",\n",
       " \"{'date': -3591820800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8309000000000001, 'title': 'The Northern Daily Times.', 'score': 0.7093901038}\",\n",
       " \"{'date': -3590784000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.722, 'title': 'The Northern Daily Times.', 'score': 0.6088651419000001}\",\n",
       " \"{'date': -3590697600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9618000000000001, 'title': 'The Northern Daily Times.', 'score': 0.5080327988000001}\",\n",
       " \"{'date': -3589488000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9399000000000001, 'title': 'The Northern Daily Times.', 'score': 0.9869059920000001}\",\n",
       " \"{'date': -3589401600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9909, 'title': 'The Northern Daily Times.', 'score': 0.9846551418}\",\n",
       " \"{'date': -3589142400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9802000000000001, 'title': 'The Northern Daily Times.', 'score': 0.9895780087}\",\n",
       " \"{'date': -3589056000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9841000000000001, 'title': 'The Northern Daily Times.', 'score': 0.9929869771000001}\",\n",
       " \"{'date': -3588969600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9801000000000001, 'title': 'The Northern Daily Times.', 'score': 0.9946441650000001}\",\n",
       " \"{'date': -3588796800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9643, 'title': 'The Northern Daily Times.', 'score': 0.9869806170000001}\",\n",
       " \"{'date': -3588451200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.7468, 'title': 'The Northern Daily Times.', 'score': 0.9851269126000001}\",\n",
       " \"{'date': -3587932800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8037000000000001, 'title': 'The Northern Daily Times.', 'score': 0.9936675429}\",\n",
       " \"{'date': -3587760000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9779, 'title': 'The Northern Daily Times.', 'score': 0.9918483496}\",\n",
       " \"{'date': -3587241600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9678, 'title': 'The Northern Daily Times.', 'score': 0.9897683859}\",\n",
       " \"{'date': -3586550400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9703, 'title': 'The Northern Daily Times.', 'score': 0.988059938}\",\n",
       " \"{'date': -3586291200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.7602, 'title': 'The Northern Daily Times.', 'score': 0.9894227386000001}\",\n",
       " \"{'date': -3584649600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9123, 'title': 'The Northern Daily Times.', 'score': 0.9934424162000001}\",\n",
       " \"{'date': -3666902400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.7636000000000001, 'title': 'The Northern Daily Times.', 'score': 0.7705588937000001}\",\n",
       " \"{'date': -3661027200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9524, 'title': 'The Northern Daily Times.', 'score': 0.7728390694}\",\n",
       " \"{'date': -3666211200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.98, 'title': 'The Northern Daily Times.', 'score': 0.7854591608}\",\n",
       " \"{'date': -3663446400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.7273000000000001, 'title': 'The Northern Daily Times.', 'score': 0.8243499994}\",\n",
       " \"{'date': -3667161600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8601000000000001, 'title': 'The Northern Daily Times.', 'score': 0.8349408507}\",\n",
       " \"{'date': -3666470400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9751000000000001, 'title': 'The Northern Daily Times.', 'score': 0.8355789781}\",\n",
       " \"{'date': -3664137600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9737, 'title': 'The Northern Daily Times.', 'score': 0.8409079313000001}\",\n",
       " \"{'date': -3666124800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9627, 'title': 'The Northern Daily Times.', 'score': 0.8425853848}\",\n",
       " \"{'date': -3596572800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9174, 'title': 'The Northern Daily Times.', 'score': 0.9514275193}\",\n",
       " \"{'date': -3596313600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9598000000000001, 'title': 'The Northern Daily Times.', 'score': 0.9255126715}\",\n",
       " \"{'date': -3595536000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9548000000000001, 'title': 'The Northern Daily Times.', 'score': 0.8683259487}\",\n",
       " \"{'date': -3593980800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9665, 'title': 'The Northern Daily Times.', 'score': 0.8939642310000001}\",\n",
       " \"{'date': -3593548800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9022, 'title': 'The Northern Daily Times.', 'score': 0.9321549535}\",\n",
       " \"{'date': -3592944000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9132, 'title': 'The Northern Daily Times.', 'score': 0.8720635772}\",\n",
       " \"{'date': -3592598400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9575, 'title': 'The Northern Daily Times.', 'score': 0.8424184918000001}\",\n",
       " \"{'date': -3590697600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8794000000000001, 'title': 'The Northern Daily Times.', 'score': 0.8520181775000001}\",\n",
       " \"{'date': -3590611200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8874000000000001, 'title': 'The Northern Daily Times.', 'score': 0.9428084493000001}\",\n",
       " \"{'date': -3589142400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8006000000000001, 'title': 'The Northern Daily Times.', 'score': 0.9382367730000001}\",\n",
       " \"{'date': -3589056000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9557, 'title': 'The Northern Daily Times.', 'score': 0.9207633734}\",\n",
       " \"{'date': -3587932800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9529000000000001, 'title': 'The Northern Daily Times.', 'score': 0.9341889024000001}\",\n",
       " \"{'date': -3587760000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8853000000000001, 'title': 'The Northern Daily Times.', 'score': 0.8748933077000001}\",\n",
       " \"{'date': -3586723200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.964, 'title': 'The Northern Daily Times.', 'score': 0.8915164471}\",\n",
       " \"{'date': -3586550400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9499000000000001, 'title': 'The Northern Daily Times.', 'score': 0.9383918047}\",\n",
       " \"{'date': -3584304000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9705, 'title': 'The Northern Daily Times.', 'score': 0.8095239997}\",\n",
       " \"{'date': -3584217600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9161, 'title': 'The Northern Daily Times.', 'score': 0.8926917315}\",\n",
       " \"{'date': -3668889600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9781000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3668889600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9803000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3668889600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9568000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3668716800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9526, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3583872000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9701000000000001, 'title': 'The Northern Daily Times.', 'score': 0.8966788054}\",\n",
       " \"{'date': -3583699200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9782000000000001, 'title': 'The Northern Daily Times.', 'score': 0.8740882874}\",\n",
       " \"{'date': -3583526400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.93, 'title': 'The Northern Daily Times.', 'score': 0.8732326627}\",\n",
       " \"{'date': -3582835200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8041, 'title': 'The Northern Daily Times.', 'score': 0.9164032936000001}\",\n",
       " \"{'date': -3582748800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9745, 'title': 'The Northern Daily Times.', 'score': 0.8868916631}\",\n",
       " \"{'date': -3581884800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9368000000000001, 'title': 'The Northern Daily Times.', 'score': 0.6592430472}\",\n",
       " \"{'date': -3668630400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9299000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3668544000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9467000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3668544000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.93, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3667766400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9367000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3667680000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9607, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3667593600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9634, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3667075200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9623, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3666988800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8421000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3666470400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.7808, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3666384000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9612, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3666384000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9566, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3666297600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9476, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3666211200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9747, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3665779200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9629000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3665606400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9762000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3665347200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.971, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3665347200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9655, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3665347200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9814, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3665174400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.7506, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3665088000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9766, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3664742400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.7993, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3664483200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9772000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3663532800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8149000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3663273600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9799, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3663273600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9595, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3663187200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9716, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3661891200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9791000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3661718400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9786, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3661718400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9672000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3661632000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.966, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3661372800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9486, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3661113600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9430000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3661027200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9077000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3660681600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9519000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3595190400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9312, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3594844800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8263, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3592166400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9652000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3591475200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9754, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3590524800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.7567, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3589920000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9757, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3589056000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9044000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3588710400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9543, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3587932800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.7851, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3586464000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9287000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3584217600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9103, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3584131200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9646, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3583958400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8453, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3583440000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9803000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3583440000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9661000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3583008000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9636, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3582748800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9825, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3582230400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9149, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3582057600000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.7985, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3581798400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9468000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3581798400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9442, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3581798400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9297000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3581798400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9676, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3581107200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9596, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3580675200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9715, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3580588800000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9193, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3580502400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9510000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3580502400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9493, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3580502400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9642000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3580502400000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.9529000000000001, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3580416000000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8797, 'title': 'The Northern Daily Times.'}\",\n",
       " \"{'date': -3580243200000, 'location': 'Liverpool, Merseyside, England', 'ocr_quality_mean': 0.8952, 'title': 'The Northern Daily Times.'}\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b608a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2d9d6",
   "metadata": {},
   "source": [
    "##Â Classify Animacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1babf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b72c21e6c3455e8a7fb47f20d0c01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/24 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/datadrive_2/lm2/lib/python3.9/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/datadrive_2/lm2/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 557, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"/datadrive_2/lm2/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 524, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"/datadrive_2/lm2/lib/python3.9/site-packages/datasets/fingerprint.py\", line 480, in wrapper\n    out = func(self, *args, **kwargs)\n  File \"/datadrive_2/lm2/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 2756, in _map_single\n    example = apply_function_on_filtered_inputs(example, i, offset=offset)\n  File \"/datadrive_2/lm2/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 2655, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"/datadrive_2/lm2/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 2347, in decorated\n    result = f(decorated_item, *args, **kwargs)\n  File \"/tmp/ipykernel_287669/1794071599.py\", line 2, in pred_data\n    return {'st_year_sep': f'[{example[add_field]}]' + ' [SEP] ' + example['Sentence'] ,\n  File \"/datadrive_2/lm2/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 121, in __getitem__\n    value = super().__getitem__(key)\n  File \"/datadrive_2/lm2/lib/python3.9/collections/__init__.py\", line 1058, in __getitem__\n    raise KeyError(key)\nKeyError: 'Date'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpred_data\u001b[39m(example,add_field\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mst_year_sep\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[add_field]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m [SEP] \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence\u001b[39m\u001b[38;5;124m'\u001b[39m] ,\n\u001b[1;32m      3\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_sep\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(example[add_field]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m [SEP] \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence\u001b[39m\u001b[38;5;124m'\u001b[39m] ,\n\u001b[1;32m      4\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_date\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(example[add_field]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m [DATE] \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[1;32m      5\u001b[0m         \n\u001b[1;32m      6\u001b[0m     }\n\u001b[0;32m----> 8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/datasets/arrow_dataset.py:2500\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2495\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   2496\u001b[0m             \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m==\u001b[39m nb_of_missing_shards\n\u001b[1;32m   2497\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of missing cached shards needs to correspond to the number of `_map_single` we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre running\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2499\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m index, async_result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 2500\u001b[0m             transformed_shards[index] \u001b[38;5;241m=\u001b[39m \u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2502\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   2503\u001b[0m     transformed_shards\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   2504\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll shards have to be defined Datasets, none should still be missing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2506\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcatenating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m shards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/datadrive_2/lm2/lib/python3.9/site-packages/multiprocess/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Date'"
     ]
    }
   ],
   "source": [
    "def pred_data(example,add_field='Date'):\n",
    "    return {'st_year_sep': f'[{example[add_field]}]' + ' [SEP] ' + example['text'] ,\n",
    "     'year_sep': str(example[add_field]) + ' [SEP] ' + example['text'] ,\n",
    "     'year_date': str(example[add_field]) + ' [DATE] ' + example['text'] \n",
    "        \n",
    "    }\n",
    "    \n",
    "dataset = dataset.map(pred_data , num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "053700e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1880] [SEP] Immured in a convent, debarred from life-giving air and light, and the beauty of life, we cease to be living, feeling, thinking girls and women, we become mere ***machines*** who blindly obey the head that directs us.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['st_year_sep'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e79fd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb333c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7958b051a3e4a0e8f6266a5c17423a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lab2code = {label:i for i,label in enumerate(dataset.unique('animacy'))}\n",
    "num_labels = len(lab2code)\n",
    "dataset = dataset.map(lambda x: {'label': lab2code[x['animacy']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d278a31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'Date', 'Sentence', 'SentenceCtxt', 'SentenceId', 'TargetExpression', 'animacy', 'humanness', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "    num_rows: 594\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f61254c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(len(dataset)*.2)\n",
    "train_test = dataset.train_test_split(test_size=test_size , seed=42)\n",
    "test_set = train_test['test']\n",
    "val_size = int(len(train_test['train'])*.1)\n",
    "train_val =  train_test['train'].train_test_split(test_size=val_size,seed=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7348eace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-hmd were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-hmd and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-st-y were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-st-y and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y_masked_25 were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y_masked_25 and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y_masked_75 were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y_masked_75 and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoints = [('distilbert','distilbert-base-uncased','[SEP]','year_sep'),\n",
    "               ('hmd_distilbert','/datadrive_2/bnert-hmd','[SEP]','year_sep'),\n",
    "               ('bnert-time-st-y','/datadrive_2/bnert-time-st-y','[SEP]','st_year_sep'),\n",
    "               ('bnert-time-y','/datadrive_2/bnert-time-y','[DATE]','year_date'),\n",
    "               ('bnert-time-y_masked_25','/datadrive_2/bnert-time-y_masked_25','[DATE]','year_date'),\n",
    "               ('bnert-time-y_masked_75','/datadrive_2/bnert-time-y_masked_75','[DATE]','year_date')]\n",
    "\n",
    "model_dict = defaultdict(dict)\n",
    "for name,checkpoint, st, sent_col in checkpoints:\n",
    "    model_dict[name]['model'] = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=num_labels)\n",
    "    model_dict[name]['tokenizer'] = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    #model_dict[name]['special_token'] = st\n",
    "    model_dict[name]['sentences'] = sent_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56ef2858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'Date', 'Sentence', 'SentenceCtxt', 'SentenceId', 'TargetExpression', 'animacy', 'humanness', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 429\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'Date', 'Sentence', 'SentenceCtxt', 'SentenceId', 'TargetExpression', 'animacy', 'humanness', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 47\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13455536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Date', 'Sentence', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 429\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Date', 'Sentence', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 47\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val = train_val.remove_columns(['Unnamed: 0','SentenceCtxt', 'SentenceId', 'TargetExpression','animacy', 'humanness'])\n",
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b73f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def add_text_col(example,source):\n",
    "#    return {'text' : example[source]}\n",
    "\n",
    "def preprocess_function(examples, target_col):\n",
    "    return tokenizer(examples[target_col], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb83cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for distilbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccb67466f3c4c7ba436b138430cba62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/429 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2161422800434a2f9be7f9dfec77eee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: year_date, st_year_sep, year_sep, Date, Sentence. If year_date, st_year_sep, year_sep, Date, Sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 429\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e0d711a2ac41bf89e6f8220694c4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0. If humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0 are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 118\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for hmd_distilbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309d1b27bcd348a29cc379d3987b3e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/429 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe0661ef3c44dcfa9efe70b91ca7a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: year_date, st_year_sep, year_sep, Date, Sentence. If year_date, st_year_sep, year_sep, Date, Sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 429\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18264f7f9da446f4a3017bd7d1ea8792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0. If humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0 are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 118\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-st-y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911dc841cc5f4ae19ca6e6d7ffd10eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/429 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f1110db93f422c9476dae77804bd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: year_date, st_year_sep, year_sep, Date, Sentence. If year_date, st_year_sep, year_sep, Date, Sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 429\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aeced93b31542de8352068d5a4c1f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0. If humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0 are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 118\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204137d7026c4fed85da24d9ac6f7192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/429 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb2217008bf4007ae5947de9fabafdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: year_date, st_year_sep, year_sep, Date, Sentence. If year_date, st_year_sep, year_sep, Date, Sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 429\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96033342fe5407b87963d14c099bc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0. If humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0 are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 118\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-y_masked_25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae21999c9af4cbc93bdcf6385a2c32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/429 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1162072da49f43c6b484f6d081f7a8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: year_date, st_year_sep, year_sep, Date, Sentence. If year_date, st_year_sep, year_sep, Date, Sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 429\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31488f4af3694882a85a7f7c98b11e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0. If humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0 are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 118\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-y_masked_75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005bde7c234640beb3e938a5e6d5d632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/429 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae67fcfbc5e452fb536e00f58321e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: year_date, st_year_sep, year_sep, Date, Sentence. If year_date, st_year_sep, year_sep, Date, Sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 429\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab01501db5bb4a46b0f39a0264953108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0. If humanness, animacy, year_sep, SentenceId, st_year_sep, year_date, Date, Sentence, TargetExpression, SentenceCtxt, Unnamed: 0 are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 118\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_dict = defaultdict(dict)\n",
    "\n",
    "for name, mdict in model_dict.items():\n",
    "    print(f'Creating a model for {name}')\n",
    "    tokenizer = model_dict[name]['tokenizer']\n",
    "    model = model_dict[name]['model']\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    sent_col = model_dict[name]['sentences']\n",
    "    \n",
    "    #train_val = train_val.map(add_text_col,fn_kwargs={'source': sent_col})\n",
    "    train_val = train_val.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_val[\"train\"],\n",
    "    eval_dataset=train_val[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    \n",
    "    test_set = test_set.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    predictions = trainer.predict(test_set)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    result_dict[name]['f1_macro'] = f1_score(preds,predictions.label_ids,average='macro')\n",
    "    result_dict[name]['f1_micro'] = f1_score(preds,predictions.label_ids,average='micro')\n",
    "    result_dict[name]['accuracy']  = accuracy_score(preds,predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d444236",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(result_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3859f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "723f12c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  f1\\_macro &  f1\\_micro &  accuracy \\\\\n",
      "\\midrule\n",
      "distilbert             &     0.752 &     0.771 &     0.771 \\\\\n",
      "hmd\\_distilbert         &     0.705 &     0.737 &     0.737 \\\\\n",
      "bnert-time-st-y        &     0.724 &     0.763 &     0.763 \\\\\n",
      "bnert-time-y           &     0.724 &     0.754 &     0.754 \\\\\n",
      "bnert-time-y\\_masked\\_25 &     0.713 &     0.746 &     0.746 \\\\\n",
      "bnert-time-y\\_masked\\_75 &     0.735 &     0.763 &     0.763 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_284692/2842510611.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(results_df.round(3).to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(results_df.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b3ea2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f8bea4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('tables/classsify_pol_wo_regex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063e09c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm2",
   "language": "python",
   "name": "lm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
