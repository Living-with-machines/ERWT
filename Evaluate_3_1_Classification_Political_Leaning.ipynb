{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0f9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import DataCollatorWithPadding\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b07302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr'],\n",
       "    num_rows: 11315511\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = load_from_disk('/datadrive_2/')\n",
    "#test_data = dataset['test']\n",
    "\n",
    "cache_dir = '/datadrive_2/hf_cache/'\n",
    "dataset = load_from_disk(\"/datadrive_2/HMD_chunked_100_test/\")\n",
    "dataset #= dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a047ed",
   "metadata": {},
   "source": [
    "## Classify by Political Leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0f1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_pattern = re.compile(r'\\bliberal|\\bconservat|\\btory\\b|\\btories\\b',re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69dea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liberal', 'conservat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_pattern.findall('liberal governments do not fire their conservative political ministers minister')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2875c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sent_split(x):\n",
    "#      return {'data': [\n",
    "#                 {'sentence':s.lower(),\n",
    "#                  'length': len(s.split()),\n",
    "#                  'pol': p, 'loc':l, 'year':y, 'ocr':o,'nlp':n} \n",
    "#                      for y,p,l,o,n,t in zip(x['year'],x['pol'],x['loc'],x['ocr_quality_mean'],x['nlp'],x['text']) \n",
    "#                       for s in sent_tokenize(t) \n",
    "#                          if pol_pattern.findall(s)\n",
    "#                  ]\n",
    "#             }\n",
    "\n",
    "# test_data = dataset.map(sent_split,batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db872537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-6f3d8b9a1021878e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-b3061892df3a5bbe.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-2d79338e1133aae6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-24d2a5525e368df0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-06bf12bcec6754fc.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-a090791e087fa494.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-b005ec092fb2e6fc.arrow\n",
      "Loading cached shuffled indices for dataset at /datadrive_2/HMD_chunked_100_test/cache-576b39d14e3bb9a9.arrow\n"
     ]
    }
   ],
   "source": [
    "political_vocab = True\n",
    "if political_vocab:\n",
    "    test_data = dataset.map(lambda x: {'sentences': x['sentences'].lower()}, num_proc=6\n",
    "                               ).filter(lambda x: len(pol_pattern.findall(x['sentences'])) > 0\n",
    "                                   ).shuffle(seed=42).select(range(15000))\n",
    "else:\n",
    "    test_data = dataset.map(lambda x: {'sentences': x['sentences'].lower()}, num_proc=6).shuffle(seed=42).select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c0086b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 1865,\n",
       " 'nlp': 2194,\n",
       " 'pol': '[lib]',\n",
       " 'loc': '[london]',\n",
       " 'sentences': 'not tell what kind of a man they werefighting. how the conservatives and extreme ra-dicals could unite in supporting mr. cobbett hadbeen a puzzle to him ever since he took a part in thepolitics of oldham. the time would soon arrivewhen they would have to think of politics, and ofthe promotion of liberal principles, by which hemeant principles in accordance with the legislationof the last 30 years. after the results they had seen,he did not think that any of them would like to goback. (hear.) opponents who had fought againstthat great and good man who had lately departedfrom them, were',\n",
       " 'ocr': 0.9807}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3edfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = test_data.filter(lambda x: x['data.length'] > 25).shuffle(seed=42).select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d45648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-e90d0f0b765edf88.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-2f0fd14b8ff167b5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-fe8323efe8a187d2.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-c9daafb7635b3c9d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-78b5a10eeb251f17.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-8b198bca3e71b49c.arrow\n"
     ]
    }
   ],
   "source": [
    "def pred_data(example,add_field='year'):\n",
    "    return {'st_year_sep': f'[{example[add_field]}]' + ' [SEP] ' + example['sentences'] ,\n",
    "     'year_sep': str(example[add_field]) + ' [SEP] ' + example['sentences'] ,\n",
    "     'year_date': str(example[add_field]) + ' [DATE] ' + example['sentences'],\n",
    "        \n",
    "    }\n",
    "    \n",
    "data = test_data.map(pred_data , num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad873104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a32db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-ebfbc9adbfcb7f5c.arrow\n"
     ]
    }
   ],
   "source": [
    "lab2code = {'[con]':0,'[lib]':1,'[rad]':2,'[neutr]':3,'[none]':4}\n",
    "num_labels = len(lab2code)\n",
    "data = data.map(lambda x: {'label': lab2code[x['pol']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcecd756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 1839,\n",
       " 'nlp': 2194,\n",
       " 'pol': '[lib]',\n",
       " 'loc': '[london]',\n",
       " 'sentences': \"such mea-sures, that triumphant majority which must carry themthrough every species of tory tribulatinn..—(cheer.s.)these are the views that i advocate—this theorpoislie.iytthat i would pursue. is it a dangerous.poliey,not recommended. by its moderation, by its good sense.?and if so, are we to support that government whichopposed to its recomnition or do you sanction hostility.to a government 'which 'will not adopt those.views,even though it may lead to a catastrophe of which theeffects are speculative ? i consider the prospect beforeus is bright, and that we have only to pursue a steadycourse of well doing, and that:your representatives oughtto enforce such sentiments as\",\n",
       " 'ocr': 0.9769,\n",
       " 'st_year_sep': \"[1839] [SEP] such mea-sures, that triumphant majority which must carry themthrough every species of tory tribulatinn..—(cheer.s.)these are the views that i advocate—this theorpoislie.iytthat i would pursue. is it a dangerous.poliey,not recommended. by its moderation, by its good sense.?and if so, are we to support that government whichopposed to its recomnition or do you sanction hostility.to a government 'which 'will not adopt those.views,even though it may lead to a catastrophe of which theeffects are speculative ? i consider the prospect beforeus is bright, and that we have only to pursue a steadycourse of well doing, and that:your representatives oughtto enforce such sentiments as\",\n",
       " 'year_sep': \"1839 [SEP] such mea-sures, that triumphant majority which must carry themthrough every species of tory tribulatinn..—(cheer.s.)these are the views that i advocate—this theorpoislie.iytthat i would pursue. is it a dangerous.poliey,not recommended. by its moderation, by its good sense.?and if so, are we to support that government whichopposed to its recomnition or do you sanction hostility.to a government 'which 'will not adopt those.views,even though it may lead to a catastrophe of which theeffects are speculative ? i consider the prospect beforeus is bright, and that we have only to pursue a steadycourse of well doing, and that:your representatives oughtto enforce such sentiments as\",\n",
       " 'year_date': \"1839 [DATE] such mea-sures, that triumphant majority which must carry themthrough every species of tory tribulatinn..—(cheer.s.)these are the views that i advocate—this theorpoislie.iytthat i would pursue. is it a dangerous.poliey,not recommended. by its moderation, by its good sense.?and if so, are we to support that government whichopposed to its recomnition or do you sanction hostility.to a government 'which 'will not adopt those.views,even though it may lead to a catastrophe of which theeffects are speculative ? i consider the prospect beforeus is bright, and that we have only to pursue a steadycourse of well doing, and that:your representatives oughtto enforce such sentiments as\",\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4c0efbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /datadrive_2/HMD_chunked_100_test/cache-0183f1f5fa427ac2.arrow and /datadrive_2/HMD_chunked_100_test/cache-20456e9c23bc1a6c.arrow\n",
      "Loading cached split indices for dataset at /datadrive_2/HMD_chunked_100_test/cache-ee6a50bc1b107305.arrow and /datadrive_2/HMD_chunked_100_test/cache-078c3d4a3ccd6457.arrow\n"
     ]
    }
   ],
   "source": [
    "test_size = int(len(data)*.2)\n",
    "train_test = data.train_test_split(test_size=test_size, seed=1984)\n",
    "test_set = train_test['test']\n",
    "val_size = int(len(train_test['train'])*.15)\n",
    "train_val =  train_test['train'].train_test_split(test_size=val_size, seed=1984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f7f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 10200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "495b21a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /datadrive_2/bnert-combined were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-combined and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoints = [#('distilbert','distilbert-base-uncased','[SEP]','year_sep'),\n",
    "               #('hmd_distilbert','/datadrive_2/bnert-hmd','[SEP]','year_sep'),\n",
    "               #('bnert-time-st-y','/datadrive_2/bnert-time-st-y','[SEP]','st_year_sep'),\n",
    "               #('bnert-time-y','/datadrive_2/bnert-time-y','[DATE]','year_date'),\n",
    "               #('bnert-time-y_masked_25','/datadrive_2/bnert-time-y_masked_25','[DATE]','year_date'),\n",
    "               #('bnert-time-y_masked_75','/datadrive_2/bnert-time-y_masked_75','[DATE]','year_date'),\n",
    "               #('bnert-pol-st','/datadrive_2/bnert-pol-st','[SEP]','year_sep'),\n",
    "               #('bnert-pol','/datadrive_2/bnert-pol','[SEP]','year_sep')\n",
    "                ('bnert-comb','/datadrive_2/bnert-combined','[POL]','year_sep')\n",
    "              ]\n",
    "\n",
    "model_dict = defaultdict(dict)\n",
    "for name,checkpoint, st, sent_col in checkpoints:\n",
    "    model_dict[name]['model'] = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=num_labels)\n",
    "    model_dict[name]['tokenizer'] = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    #model_dict[name]['special_token'] = st\n",
    "    model_dict[name]['sentences'] = sent_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44f12775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['year', 'pol', 'sentences', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 10200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['year', 'pol', 'sentences', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val = train_val.remove_columns(['nlp', 'ocr', 'loc'])\n",
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6fed10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def add_text_col(example,source):\n",
    "#    return {'text' : example[source]}\n",
    "\n",
    "def preprocess_function(examples, target_col):\n",
    "    return tokenizer(examples[target_col], truncation=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07e895a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-comb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8718bb575c5b465984a7022a009b8a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3010d32e23647cba5fcfb117059abc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: st_year_sep, pol, year_sep, year_date, sentences, year. If st_year_sep, pol, year_sep, year_date, sentences, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 19:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.174100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.123100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-comb/checkpoint-500\n",
      "Configuration saved in ./results_bnert-comb/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-comb/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-comb/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-comb/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-comb/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-comb/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-comb/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-comb/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-comb/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-comb/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-comb/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-comb/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-comb/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-comb/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-comb/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-comb/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-comb/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-comb/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-comb/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-comb/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-comb/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-comb/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-comb/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-comb/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-comb/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-comb/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-comb/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-comb/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-comb/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /datadrive_2/bnert-comb-pol/config.json\n",
      "Model weights saved in /datadrive_2/bnert-comb-pol/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/bnert-comb-pol/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/bnert-comb-pol/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10932d9b8b44de38ec053bc602c106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: st_year_sep, loc, pol, year_sep, ocr, sentences, year_date, nlp, year. If st_year_sep, loc, pol, year_sep, ocr, sentences, year_date, nlp, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_dict = defaultdict(dict)\n",
    "\n",
    "for name, mdict in model_dict.items():\n",
    "    print(f'Creating a model for {name}')\n",
    "    tokenizer = model_dict[name]['tokenizer']\n",
    "    model = model_dict[name]['model']\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    sent_col = model_dict[name]['sentences']\n",
    "    \n",
    "    #train_val = train_val.map(add_text_col,fn_kwargs={'source': sent_col})\n",
    "    train_val = train_val.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "    seed=1984,\n",
    "    output_dir=f\"./results_{name}\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_val[\"train\"],\n",
    "    eval_dataset=train_val[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    model.save_pretrained(f'/datadrive_2/{name}-pol')\n",
    "    tokenizer.save_pretrained(f\"/datadrive_2/{name}-pol\")\n",
    "    \n",
    "    test_set = test_set.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    predictions = trainer.predict(test_set)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    result_dict[name]['f1_macro'] = f1_score(preds,predictions.label_ids,average='macro')\n",
    "    result_dict[name]['f1_micro'] = f1_score(preds,predictions.label_ids,average='micro')\n",
    "    result_dict[name]['accuracy']  = accuracy_score(preds,predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52123841",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(result_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60d00595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  f1\\_macro &  f1\\_micro &  accuracy \\\\\n",
      "\\midrule\n",
      "bnert-comb &     0.691 &      0.85 &      0.85 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1474668/2842510611.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(results_df.round(3).to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(results_df.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61ebcd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('tables/classsify_pol_regex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b78a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm2",
   "language": "python",
   "name": "lm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
