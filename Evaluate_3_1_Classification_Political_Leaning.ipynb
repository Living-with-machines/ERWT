{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0f9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import DataCollatorWithPadding\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b07302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr'],\n",
       "    num_rows: 11315511\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = load_from_disk('/datadrive_2/')\n",
    "#test_data = dataset['test']\n",
    "\n",
    "cache_dir = '/datadrive_2/hf_cache/'\n",
    "dataset = load_from_disk(\"/datadrive_2/HMD_chunked_100_test/\")\n",
    "dataset #= dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a047ed",
   "metadata": {},
   "source": [
    "##Â Classify by Political Leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b0f1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_pattern = re.compile(r'\\bliberal|\\bconservat|\\btory\\b|\\btories\\b',re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69dea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liberal', 'conservat']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_pattern.findall('liberal governments do not fire their conservative political ministers minister')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2875c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-ba07e195c8ec9d10.arrow\n"
     ]
    }
   ],
   "source": [
    "# def sent_split(x):\n",
    "#      return {'data': [\n",
    "#                 {'sentence':s.lower(),\n",
    "#                  'length': len(s.split()),\n",
    "#                  'pol': p, 'loc':l, 'year':y, 'ocr':o,'nlp':n} \n",
    "#                      for y,p,l,o,n,t in zip(x['year'],x['pol'],x['loc'],x['ocr_quality_mean'],x['nlp'],x['text']) \n",
    "#                       for s in sent_tokenize(t) \n",
    "#                          if pol_pattern.findall(s)\n",
    "#                  ]\n",
    "#             }\n",
    "\n",
    "# test_data = dataset.map(sent_split,batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db872537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-6f3d8b9a1021878e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-2d79338e1133aae6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-24d2a5525e368df0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-b3061892df3a5bbe.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-a090791e087fa494.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-06bf12bcec6754fc.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1c8369ea99440ca309ede9b3d6cfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11316 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "political_vocab = True\n",
    "if political_vocab:\n",
    "    test_data = dataset.map(lambda x: {'sentences': x['sentences'].lower()}, num_proc=6\n",
    "                               ).filter(lambda x: len(pol_pattern.findall(x['sentences'])) > 0\n",
    "                                   ).shuffle(seed=42).select(range(15000))\n",
    "else:\n",
    "    test_data = dataset.map(lambda x: {'sentences': x['sentences'].lower()}, num_proc=6).shuffle(seed=42).select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c0086b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 1865,\n",
       " 'nlp': 2194,\n",
       " 'pol': '[lib]',\n",
       " 'loc': '[london]',\n",
       " 'sentences': 'not tell what kind of a man they werefighting. how the conservatives and extreme ra-dicals could unite in supporting mr. cobbett hadbeen a puzzle to him ever since he took a part in thepolitics of oldham. the time would soon arrivewhen they would have to think of politics, and ofthe promotion of liberal principles, by which hemeant principles in accordance with the legislationof the last 30 years. after the results they had seen,he did not think that any of them would like to goback. (hear.) opponents who had fought againstthat great and good man who had lately departedfrom them, were',\n",
       " 'ocr': 0.9807}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de3edfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = test_data.filter(lambda x: x['data.length'] > 25).shuffle(seed=42).select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d45648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48679e24509645c0ab1af822a5037a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122ace8b226a486b8ad7c6340a23edd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c45b1ecdd244e8e9b63ea79b8032b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a61947c3ae47dd9d5b80757a6ce145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30140f714d754bb1bc98e393334e8fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e516e62fe4b466ea4fa839e04fc5cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pred_data(example,add_field='year'):\n",
    "    return {'st_year_sep': f'[{example[add_field]}]' + ' [SEP] ' + example['sentences'] ,\n",
    "     'year_sep': str(example[add_field]) + ' [SEP] ' + example['sentences'] ,\n",
    "     'year_date': str(example[add_field]) + ' [DATE] ' + example['sentences'],\n",
    "        \n",
    "    }\n",
    "    \n",
    "data = test_data.map(pred_data , num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad873104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5a32db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4e3b9697db4b329893ef9647a5b961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lab2code = {'[con]':0,'[lib]':1,'[rad]':2,'[neutr]':3,'[none]':4}\n",
    "num_labels = len(lab2code)\n",
    "data = data.map(lambda x: {'label': lab2code[x['pol']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcecd756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 1839,\n",
       " 'nlp': 2194,\n",
       " 'pol': '[lib]',\n",
       " 'loc': '[london]',\n",
       " 'sentences': \"such mea-sures, that triumphant majority which must carry themthrough every species of tory tribulatinn..â(cheer.s.)these are the views that i advocateâthis theorpoislie.iytthat i would pursue. is it a dangerous.poliey,not recommended. by its moderation, by its good sense.?and if so, are we to support that government whichopposed to its recomnition or do you sanction hostility.to a government 'which 'will not adopt those.views,even though it may lead to a catastrophe of which theeffects are speculative ? i consider the prospect beforeus is bright, and that we have only to pursue a steadycourse of well doing, and that:your representatives oughtto enforce such sentiments as\",\n",
       " 'ocr': 0.9769,\n",
       " 'st_year_sep': \"[1839] [SEP] such mea-sures, that triumphant majority which must carry themthrough every species of tory tribulatinn..â(cheer.s.)these are the views that i advocateâthis theorpoislie.iytthat i would pursue. is it a dangerous.poliey,not recommended. by its moderation, by its good sense.?and if so, are we to support that government whichopposed to its recomnition or do you sanction hostility.to a government 'which 'will not adopt those.views,even though it may lead to a catastrophe of which theeffects are speculative ? i consider the prospect beforeus is bright, and that we have only to pursue a steadycourse of well doing, and that:your representatives oughtto enforce such sentiments as\",\n",
       " 'year_sep': \"1839 [SEP] such mea-sures, that triumphant majority which must carry themthrough every species of tory tribulatinn..â(cheer.s.)these are the views that i advocateâthis theorpoislie.iytthat i would pursue. is it a dangerous.poliey,not recommended. by its moderation, by its good sense.?and if so, are we to support that government whichopposed to its recomnition or do you sanction hostility.to a government 'which 'will not adopt those.views,even though it may lead to a catastrophe of which theeffects are speculative ? i consider the prospect beforeus is bright, and that we have only to pursue a steadycourse of well doing, and that:your representatives oughtto enforce such sentiments as\",\n",
       " 'year_date': \"1839 [DATE] such mea-sures, that triumphant majority which must carry themthrough every species of tory tribulatinn..â(cheer.s.)these are the views that i advocateâthis theorpoislie.iytthat i would pursue. is it a dangerous.poliey,not recommended. by its moderation, by its good sense.?and if so, are we to support that government whichopposed to its recomnition or do you sanction hostility.to a government 'which 'will not adopt those.views,even though it may lead to a catastrophe of which theeffects are speculative ? i consider the prospect beforeus is bright, and that we have only to pursue a steadycourse of well doing, and that:your representatives oughtto enforce such sentiments as\",\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4c0efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(len(data)*.2)\n",
    "train_test = data.train_test_split(test_size=test_size, seed=1984)\n",
    "test_set = train_test['test']\n",
    "val_size = int(len(train_test['train'])*.15)\n",
    "train_val =  train_test['train'].train_test_split(test_size=val_size, seed=1984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5f7f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 10200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "495b21a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-hmd were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-hmd and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-st-y were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-st-y and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y_masked_25 were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y_masked_25 and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y_masked_75 were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y_masked_75 and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-pol-st were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-pol-st and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-pol were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-pol and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoints = [('distilbert','distilbert-base-uncased','[SEP]','year_sep'),\n",
    "               ('hmd_distilbert','/datadrive_2/bnert-hmd','[SEP]','year_sep'),\n",
    "               ('bnert-time-st-y','/datadrive_2/bnert-time-st-y','[SEP]','st_year_sep'),\n",
    "               ('bnert-time-y','/datadrive_2/bnert-time-y','[DATE]','year_date'),\n",
    "               ('bnert-time-y_masked_25','/datadrive_2/bnert-time-y_masked_25','[DATE]','year_date'),\n",
    "               ('bnert-time-y_masked_75','/datadrive_2/bnert-time-y_masked_75','[DATE]','year_date'),\n",
    "               ('bnert-pol-st','/datadrive_2/bnert-pol-st','[SEP]','year_sep'),\n",
    "               ('bnert-pol','/datadrive_2/bnert-pol','[SEP]','year_sep')]\n",
    "\n",
    "model_dict = defaultdict(dict)\n",
    "for name,checkpoint, st, sent_col in checkpoints:\n",
    "    model_dict[name]['model'] = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=num_labels)\n",
    "    model_dict[name]['tokenizer'] = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    #model_dict[name]['special_token'] = st\n",
    "    model_dict[name]['sentences'] = sent_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44f12775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['year', 'pol', 'sentences', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 10200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['year', 'pol', 'sentences', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val = train_val.remove_columns(['nlp', 'ocr', 'loc'])\n",
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6fed10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def add_text_col(example,source):\n",
    "#    return {'text' : example[source]}\n",
    "\n",
    "def preprocess_function(examples, target_col):\n",
    "    return tokenizer(examples[target_col], truncation=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e895a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for distilbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e9c1a863da4b5696fd2aad0f849b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c4bd5879db475999ca7853e11fce5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: year_date, st_year_sep, sentences, year, year_sep, pol. If year_date, st_year_sep, sentences, year, year_sep, pol are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 11:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.436500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.155400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_distilbert/checkpoint-500\n",
      "Configuration saved in ./results_distilbert/checkpoint-500/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-1000\n",
      "Configuration saved in ./results_distilbert/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-1500\n",
      "Configuration saved in ./results_distilbert/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-2000\n",
      "Configuration saved in ./results_distilbert/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-2500\n",
      "Configuration saved in ./results_distilbert/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-3000\n",
      "Configuration saved in ./results_distilbert/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /datadrive_2/distilbert-pol/config.json\n",
      "Model weights saved in /datadrive_2/distilbert-pol/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/distilbert-pol/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/distilbert-pol/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4be6f7445864669aee2aa6657cd2eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: loc, year_date, st_year_sep, pol, sentences, year, year_sep, nlp, ocr. If loc, year_date, st_year_sep, pol, sentences, year, year_sep, nlp, ocr are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for hmd_distilbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4728e71c64c41ff8f13375597d3bc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee0acfd3e5441e1944e90462b4ae7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: year_date, st_year_sep, sentences, year, year_sep, pol. If year_date, st_year_sep, sentences, year, year_sep, pol are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 11:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.404500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.324600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.136100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-500\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-500/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-1000\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-1500\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-2000\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-2500\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-3000\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /datadrive_2/hmd_distilbert-pol/config.json\n",
      "Model weights saved in /datadrive_2/hmd_distilbert-pol/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/hmd_distilbert-pol/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/hmd_distilbert-pol/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2fa86bae4945eea1afc6d87b344031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: loc, year_date, st_year_sep, pol, sentences, year, year_sep, nlp, ocr. If loc, year_date, st_year_sep, pol, sentences, year, year_sep, nlp, ocr are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-st-y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67cee173a5f4cad9692d8c38250c50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603423fde2d04c2485307fefe7a6b90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: year_date, st_year_sep, sentences, year, year_sep, pol. If year_date, st_year_sep, sentences, year, year_sep, pol are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1616' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1616/3190 05:44 < 05:35, 4.69 it/s, Epoch 2.53/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.617600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.339800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-500\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-1500/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "result_dict = defaultdict(dict)\n",
    "\n",
    "for name, mdict in model_dict.items():\n",
    "    print(f'Creating a model for {name}')\n",
    "    tokenizer = model_dict[name]['tokenizer']\n",
    "    model = model_dict[name]['model']\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    sent_col = model_dict[name]['sentences']\n",
    "    \n",
    "    #train_val = train_val.map(add_text_col,fn_kwargs={'source': sent_col})\n",
    "    train_val = train_val.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "    seed=1984,\n",
    "    output_dir=f\"./results_{name}\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_val[\"train\"],\n",
    "    eval_dataset=train_val[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    model.save_pretrained(f'/datadrive_2/{name}-pol')\n",
    "    tokenizer.save_pretrained(f\"/datadrive_2/{name}-pol\")\n",
    "    \n",
    "    test_set = test_set.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    predictions = trainer.predict(test_set)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    result_dict[name]['f1_macro'] = f1_score(preds,predictions.label_ids,average='macro')\n",
    "    result_dict[name]['f1_micro'] = f1_score(preds,predictions.label_ids,average='micro')\n",
    "    result_dict[name]['accuracy']  = accuracy_score(preds,predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52123841",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(result_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d00595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebcd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('tables/classsify_pol_regex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b78a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm2",
   "language": "python",
   "name": "lm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
