{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0f9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import DataCollatorWithPadding\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b07302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_from_disk('/datadrive_2/')\n",
    "#test_data = dataset['test']\n",
    "\n",
    "cache_dir = '/datadrive_2/hf_cache/'\n",
    "dataset = load_from_disk(\"/datadrive_2/HMD_context\")\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2a7772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'title', 'location', 'date', 'ocr_quality_mean', 'year', 'length', 'month', 'nlp', 'pol', 'loc'],\n",
       "    num_rows: 2787067\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a047ed",
   "metadata": {},
   "source": [
    "## Classify by Political Leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0f1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_pattern = re.compile(r'\\bliberal|\\bconservat|\\bministers?|\\btory\\b|\\btories\\b|\\bgovernments?|\\bpolitic',re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e69dea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liberal', 'governments', 'politic', 'ministers', 'minister']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_pattern.findall('liberal governments do not fire their political ministers minister')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2875c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-ba07e195c8ec9d10.arrow\n"
     ]
    }
   ],
   "source": [
    "def sent_split(x):\n",
    "     return {'data': [\n",
    "                {'sentence':s.lower(),\n",
    "                 'length': len(s.split()),\n",
    "                 'pol': p, 'loc':l, 'year':y, 'ocr':o,'nlp':n} \n",
    "                     for y,p,l,o,n,t in zip(x['year'],x['pol'],x['loc'],x['ocr_quality_mean'],x['nlp'],x['text']) \n",
    "                      for s in sent_tokenize(t) \n",
    "                         if pol_pattern.findall(s)\n",
    "                 ]\n",
    "            }\n",
    "\n",
    "test_data = dataset.map(sent_split,batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db872537",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0086b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data.length': 90,\n",
       " 'data.loc': '[liverpool]',\n",
       " 'data.nlp': 2083,\n",
       " 'data.ocr': 0.985,\n",
       " 'data.pol': '[neutr]',\n",
       " 'data.sentence': \"but we must yet be on the alert.her majesty's government has, with praise-worthy alacrity, already directed, by an order incouncil, that the provisions of the act for the pre-vention of epidemic, indemic, and contagious dis-eases, be immediately put in force through thewhole of great britain; and liverpool is not behind-hand in giving effect to so necessary, so indispen-sable a precaution.the very attention which has in this emergencyto be given to such a subject as that of the publichealth, may result in a permanent improvement ofthe sanitary state of our town.\",\n",
       " 'data.year': 1853}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de3edfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-a30da7bf93707de5.arrow\n",
      "Loading cached shuffled indices for dataset at /datadrive_2/HMD_context/train/cache-ef1d6e941b6c7082.arrow\n"
     ]
    }
   ],
   "source": [
    "data = test_data.filter(lambda x: x['data.length'] > 25).shuffle(seed=42).select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d45648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-f8d0db47c344d25a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-798f6926ed263cdc.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-673e9f15eeac8d91.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-0552d9181d587ab8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-8053603b608b107e.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-67771e8f0317eefe.arrow\n"
     ]
    }
   ],
   "source": [
    "def pred_data(example,add_field='data.year'):\n",
    "    return {'st_year_sep': f'[{example[add_field]}]' + ' [SEP] ' + example['data.sentence'] ,\n",
    "     'year_sep': str(example[add_field]) + ' [SEP] ' + example['data.sentence'] ,\n",
    "     'year_date': str(example[add_field]) + ' [DATE] ' + example['data.sentence'],\n",
    "        \n",
    "    }\n",
    "    \n",
    "data = data.map(pred_data , num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad873104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a32db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-29691a666b941845.arrow\n"
     ]
    }
   ],
   "source": [
    "lab2code = {'[con]':0,'[lib]':1,'[rad]':2,'[neutr]':3,'[none]':4}\n",
    "num_labels = len(lab2code)\n",
    "data = data.map(lambda x: {'label': lab2code[x['data.pol']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcecd756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data.length': 120,\n",
       " 'data.loc': '[london]',\n",
       " 'data.nlp': 2194,\n",
       " 'data.ocr': 0.9156,\n",
       " 'data.pol': '[lib]',\n",
       " 'data.sentence': \"l'pon payment of thearrear of the premiums, the assured will be entitled to a fall parti-cipation of the profits.the assured may at all times borrow an amount equal to the valueof the policy, on its security.in all the transactions of the company the utmost liberality willbe exercised.life assurances will be effected with all the advantages to theassured afforded by the most respectable established companies.a table of premiums will shortly be published.tontine annuities.- • -who from the hopethis company, in subinittini the subject of tontine annuities tothe public, is intttenegdenbgyevtihtey,baenlideftntehadtestihreereer aearesemanandyafflperasersiee,as they progress throughb life, will gladly avail themselves of thism .d eaonfai nivneesitemaserntg;---inzrawhich they will secure a large remunera-tiveooffttheir nominee.guaranteed to them by the companyduring their own lives or theb.1001.\",\n",
       " 'data.year': 1846,\n",
       " 'st_year_sep': \"[1846] [SEP] l'pon payment of thearrear of the premiums, the assured will be entitled to a fall parti-cipation of the profits.the assured may at all times borrow an amount equal to the valueof the policy, on its security.in all the transactions of the company the utmost liberality willbe exercised.life assurances will be effected with all the advantages to theassured afforded by the most respectable established companies.a table of premiums will shortly be published.tontine annuities.- • -who from the hopethis company, in subinittini the subject of tontine annuities tothe public, is intttenegdenbgyevtihtey,baenlideftntehadtestihreereer aearesemanandyafflperasersiee,as they progress throughb life, will gladly avail themselves of thism .d eaonfai nivneesitemaserntg;---inzrawhich they will secure a large remunera-tiveooffttheir nominee.guaranteed to them by the companyduring their own lives or theb.1001.\",\n",
       " 'year_sep': \"1846 [SEP] l'pon payment of thearrear of the premiums, the assured will be entitled to a fall parti-cipation of the profits.the assured may at all times borrow an amount equal to the valueof the policy, on its security.in all the transactions of the company the utmost liberality willbe exercised.life assurances will be effected with all the advantages to theassured afforded by the most respectable established companies.a table of premiums will shortly be published.tontine annuities.- • -who from the hopethis company, in subinittini the subject of tontine annuities tothe public, is intttenegdenbgyevtihtey,baenlideftntehadtestihreereer aearesemanandyafflperasersiee,as they progress throughb life, will gladly avail themselves of thism .d eaonfai nivneesitemaserntg;---inzrawhich they will secure a large remunera-tiveooffttheir nominee.guaranteed to them by the companyduring their own lives or theb.1001.\",\n",
       " 'year_date': \"1846 [DATE] l'pon payment of thearrear of the premiums, the assured will be entitled to a fall parti-cipation of the profits.the assured may at all times borrow an amount equal to the valueof the policy, on its security.in all the transactions of the company the utmost liberality willbe exercised.life assurances will be effected with all the advantages to theassured afforded by the most respectable established companies.a table of premiums will shortly be published.tontine annuities.- • -who from the hopethis company, in subinittini the subject of tontine annuities tothe public, is intttenegdenbgyevtihtey,baenlideftntehadtestihreereer aearesemanandyafflperasersiee,as they progress throughb life, will gladly avail themselves of thism .d eaonfai nivneesitemaserntg;---inzrawhich they will secure a large remunera-tiveooffttheir nominee.guaranteed to them by the companyduring their own lives or theb.1001.\",\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4c0efbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /datadrive_2/HMD_context/train/cache-1445f72e23a5c03c.arrow and /datadrive_2/HMD_context/train/cache-b3b647c8217a2d35.arrow\n",
      "Loading cached split indices for dataset at /datadrive_2/HMD_context/train/cache-bae83ef1b0045f65.arrow and /datadrive_2/HMD_context/train/cache-c691a7871720a3af.arrow\n"
     ]
    }
   ],
   "source": [
    "test_size = int(len(data)*.2)\n",
    "train_test = data.train_test_split(test_size=test_size, seed=1984)\n",
    "test_set = train_test['test']\n",
    "val_size = int(len(train_test['train'])*.15)\n",
    "train_val =  train_test['train'].train_test_split(test_size=val_size, seed=1984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5f7f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['data.length', 'data.loc', 'data.nlp', 'data.ocr', 'data.pol', 'data.sentence', 'data.year', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 10200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['data.length', 'data.loc', 'data.nlp', 'data.ocr', 'data.pol', 'data.sentence', 'data.year', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "495b21a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-hmd were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-hmd and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-st-y were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-st-y and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y_masked_25 were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y_masked_25 and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y_masked_75 were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y_masked_75 and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-pol-st were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-pol-st and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-pol were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-pol and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoints = [('distilbert','distilbert-base-uncased','[SEP]','year_sep'),\n",
    "               ('hmd_distilbert','/datadrive_2/bnert-hmd','[SEP]','year_sep'),\n",
    "               ('bnert-time-st-y','/datadrive_2/bnert-time-st-y','[SEP]','st_year_sep'),\n",
    "               ('bnert-time-y','/datadrive_2/bnert-time-y','[DATE]','year_date'),\n",
    "               ('bnert-time-y_masked_25','/datadrive_2/bnert-time-y_masked_25','[DATE]','year_date'),\n",
    "               ('bnert-time-y_masked_75','/datadrive_2/bnert-time-y_masked_75','[DATE]','year_date'),\n",
    "               ('bnert-pol-st','/datadrive_2/bnert-pol-st','[SEP]','year_sep'),\n",
    "               ('bnert-pol','/datadrive_2/bnert-pol','[SEP]','year_sep')]\n",
    "\n",
    "model_dict = defaultdict(dict)\n",
    "for name,checkpoint, st, sent_col in checkpoints:\n",
    "    model_dict[name]['model'] = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=num_labels)\n",
    "    model_dict[name]['tokenizer'] = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    #model_dict[name]['special_token'] = st\n",
    "    model_dict[name]['sentences'] = sent_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44f12775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['data.pol', 'data.sentence', 'data.year', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 10200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['data.pol', 'data.sentence', 'data.year', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val = train_val.remove_columns(['data.nlp', 'data.ocr','data.length', 'data.loc'])\n",
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6fed10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def add_text_col(example,source):\n",
    "#    return {'text' : example[source]}\n",
    "\n",
    "def preprocess_function(examples, target_col):\n",
    "    return tokenizer(examples[target_col], truncation=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07e895a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for distilbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed69515c18b49bc9a39da41c61fc0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bc4aad43234048b97d3db733973eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date. If data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 13:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.595700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.457300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.196800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_distilbert/checkpoint-500\n",
      "Configuration saved in ./results_distilbert/checkpoint-500/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-1000\n",
      "Configuration saved in ./results_distilbert/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-1500\n",
      "Configuration saved in ./results_distilbert/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-2000\n",
      "Configuration saved in ./results_distilbert/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-2500\n",
      "Configuration saved in ./results_distilbert/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-3000\n",
      "Configuration saved in ./results_distilbert/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964d28a6017c440b988f60c09a61fb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp. If data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for hmd_distilbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23dfd33abf84746ad5771b36ce3a077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4055c695c72342588bbe1ffeef8cd0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date. If data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 14:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.596900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.441600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.308100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.174200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-500\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-500/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-1000\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-1500\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-2000\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-2500\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-3000\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87951d3c3a364ceea56d90e300dcbea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp. If data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-st-y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c4f62ec4554054a0ebcb7cbc7e6432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893395995d6f42f1b5feb6dfa0687388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date. If data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 13:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.448100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.169300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-500\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996b835a52ae489cb4c87a76b3e25fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp. If data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84805c4c3b33478e8812eb1a20145c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d637366bb25b47c99a1291fe57be0eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date. If data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 13:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.436100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.160700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-500\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a717d06ad640eca17786ed08253358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp. If data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-y_masked_25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b8c66a60ce4eb2aacfb13ebb0376b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcb1873aafe406b99544a3d2d0210ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date. If data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 12:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.432700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.156100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-500\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d78b672b6c408dae61f07463d958a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp. If data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-y_masked_75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73033c6cd4c4f158b060c538b5f042c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfc1ba1a6504319bd0638058e2f5ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date. If data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 12:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.156300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-500\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7383e34c5504181bac1c21ba9320094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp. If data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-pol-st\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144b699ba7be45cc861981082ce32d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d834711bb6e248d8b99b98e592acc108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date. If data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 12:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.581200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.367700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.302800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.171900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-500\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767ad214018c41319b8def0811f0b7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp. If data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-pol\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10fc01fbba2400096ea873333b57482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c77a137b1a34d4f84b978264159196c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date. If data.year, year_sep, data.sentence, st_year_sep, data.pol, year_date are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 12:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.169500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-500\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae68da2ad894233b0167ec913961c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp. If data.length, data.year, year_sep, data.sentence, data.loc, st_year_sep, data.pol, year_date, data.ocr, data.nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_dict = defaultdict(dict)\n",
    "\n",
    "for name, mdict in model_dict.items():\n",
    "    print(f'Creating a model for {name}')\n",
    "    tokenizer = model_dict[name]['tokenizer']\n",
    "    model = model_dict[name]['model']\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    sent_col = model_dict[name]['sentences']\n",
    "    \n",
    "    #train_val = train_val.map(add_text_col,fn_kwargs={'source': sent_col})\n",
    "    train_val = train_val.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "    seed=1984,\n",
    "    output_dir=f\"./results_{name}\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_val[\"train\"],\n",
    "    eval_dataset=train_val[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    model.save_pretrained(f'/datadrive_2/{name}-pol')\n",
    "    tokenizer.save_pretrained(f\"/datadrive_2/{name}-pol\")\n",
    "    \n",
    "    test_set = test_set.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    predictions = trainer.predict(test_set)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    result_dict[name]['f1_macro'] = f1_score(preds,predictions.label_ids,average='macro')\n",
    "    result_dict[name]['f1_micro'] = f1_score(preds,predictions.label_ids,average='micro')\n",
    "    result_dict[name]['accuracy']  = accuracy_score(preds,predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52123841",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(result_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60d00595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  f1\\_macro &  f1\\_micro &  accuracy \\\\\n",
      "\\midrule\n",
      "distilbert             &     0.481 &     0.829 &     0.829 \\\\\n",
      "hmd\\_distilbert         &     0.549 &     0.833 &     0.833 \\\\\n",
      "bnert-time-st-y        &     0.562 &     0.835 &     0.835 \\\\\n",
      "bnert-time-y           &     0.550 &     0.843 &     0.843 \\\\\n",
      "bnert-time-y\\_masked\\_25 &     0.568 &     0.840 &     0.840 \\\\\n",
      "bnert-time-y\\_masked\\_75 &     0.523 &     0.831 &     0.831 \\\\\n",
      "bnert-pol-st           &     0.545 &     0.830 &     0.830 \\\\\n",
      "bnert-pol              &     0.545 &     0.835 &     0.835 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_397981/2842510611.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(results_df.round(3).to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(results_df.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61ebcd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('tables/classsify_pol_regex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b78a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm2",
   "language": "python",
   "name": "lm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
