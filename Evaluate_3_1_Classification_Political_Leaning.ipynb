{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0f9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import DataCollatorWithPadding\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96b07302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr'],\n",
       "    num_rows: 11315511\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = load_from_disk('/datadrive_2/')\n",
    "#test_data = dataset['test']\n",
    "\n",
    "cache_dir = '/datadrive_2/hf_cache/'\n",
    "dataset = load_from_disk(\"/datadrive_2/HMD_chunked_100_test/\")\n",
    "dataset #= dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a047ed",
   "metadata": {},
   "source": [
    "## Classify by Political Leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0f1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_pattern = re.compile(r'\\bliberal|\\bconservat|\\bministers?|\\btory\\b|\\btories\\b|\\bgovernments?|\\bpolitic',re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e69dea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liberal', 'governments', 'politic', 'ministers', 'minister']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_pattern.findall('liberal governments do not fire their political ministers minister')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2875c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-ba07e195c8ec9d10.arrow\n"
     ]
    }
   ],
   "source": [
    "# def sent_split(x):\n",
    "#      return {'data': [\n",
    "#                 {'sentence':s.lower(),\n",
    "#                  'length': len(s.split()),\n",
    "#                  'pol': p, 'loc':l, 'year':y, 'ocr':o,'nlp':n} \n",
    "#                      for y,p,l,o,n,t in zip(x['year'],x['pol'],x['loc'],x['ocr_quality_mean'],x['nlp'],x['text']) \n",
    "#                       for s in sent_tokenize(t) \n",
    "#                          if pol_pattern.findall(s)\n",
    "#                  ]\n",
    "#             }\n",
    "\n",
    "# test_data = dataset.map(sent_split,batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db872537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-2d79338e1133aae6.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-6f3d8b9a1021878e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-b3061892df3a5bbe.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-24d2a5525e368df0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-06bf12bcec6754fc.arrow\n",
      "Loading cached processed dataset at /datadrive_2/HMD_chunked_100_test/cache-a090791e087fa494.arrow\n",
      "Loading cached shuffled indices for dataset at /datadrive_2/HMD_chunked_100_test/cache-2e3366e450e9f2f2.arrow\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset.map(lambda x: {'sentences': x['sentences'].lower()}, num_proc=6).shuffle(seed=42).select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c0086b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 1853,\n",
       " 'nlp': 2194,\n",
       " 'pol': '[lib]',\n",
       " 'loc': '[london]',\n",
       " 'sentences': 'a slight natural yearning for theadmirable status quo of our constitution ? doesnot lord john feel that affection for the presenthouse of commons, with all its faults, whichprompts a natural parent to resent the interferenceof a neighbour in the education and even the ne-cessary correction of his offspring ? no father, nomother, ever yet tolerated the hand of thestranger on her child. casually, and by the way,lord john says it—but backhanders are alwaysthe strongest blows—\" it would be difficult, per-haps, to find a time in the history of any countrywhen so many measures of importance have beenobtained, without convulsion and',\n",
       " 'ocr': 0.9639}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de3edfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /datadrive_2/HMD_context/train/cache-a30da7bf93707de5.arrow\n",
      "Loading cached shuffled indices for dataset at /datadrive_2/HMD_context/train/cache-ef1d6e941b6c7082.arrow\n"
     ]
    }
   ],
   "source": [
    "#data = test_data.filter(lambda x: x['data.length'] > 25).shuffle(seed=42).select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d45648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c677041a6bd7430283e0e483793b508c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfc0290a93b420baa0d08fcf25792c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ed0a13b776491e99919b05b09e2b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9074321f98c34a62b4d15cf25fc51f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d853e08a6274a6e8146065df1ac5fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77d6ea5820a42bab3da89cf7b1f9433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/2500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pred_data(example,add_field='year'):\n",
    "    return {'st_year_sep': f'[{example[add_field]}]' + ' [SEP] ' + example['sentences'] ,\n",
    "     'year_sep': str(example[add_field]) + ' [SEP] ' + example['sentences'] ,\n",
    "     'year_date': str(example[add_field]) + ' [DATE] ' + example['sentences'],\n",
    "        \n",
    "    }\n",
    "    \n",
    "data = test_data.map(pred_data , num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad873104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5a32db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f565f5243d7d418cae61f49fe7b50b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lab2code = {'[con]':0,'[lib]':1,'[rad]':2,'[neutr]':3,'[none]':4}\n",
    "num_labels = len(lab2code)\n",
    "data = data.map(lambda x: {'label': lab2code[x['pol']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcecd756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 1859,\n",
       " 'nlp': 2084,\n",
       " 'pol': '[neutr]',\n",
       " 'loc': '[liverpool]',\n",
       " 'sentences': 'sitenirranv, roy institute as tou ry, v. ireno. i.—nervous debility, loss of kient.„2,l:oo!op orof sight, prostration of physical energy, 1;\\': asor\\'\\'business, study, or society, with dr. marston \\'01coax. men.o• heiti 0addressed specially to young 14. ...e oh 0%—marrlagerbanodbilgaltetipoltnsodanudetilrn°ooimp ten &c. \\' addressed to those who dam. 1010 . totof removing all impediments. qi thee dirpthaenß:orrrs4 2ar.cll\\'of healthful children, clearly expl.-11:91p .no. 3.—tiie great social evil. 11\\' wee voerdiseases which result from it, with da. minor ~,fc „i;,da. massrosi continues to be consulted pe ,e,:.„penned with.veoftiit.reatment, by which msactinr luso* a\\'\\'\\'s,y.o,,tifi,,:vilti9.residence,ez ens-......r5z 0 d-6 orby letter,47, eitherin at thexrßotal.lnsroittitatetßa: 1171,01\\'\"raftpaordrticeuiveargry.inf°\"\"ti°l4 70-00.',\n",
       " 'ocr': 0.643,\n",
       " 'st_year_sep': '[1859] [SEP] sitenirranv, roy institute as tou ry, v. ireno. i.—nervous debility, loss of kient.„2,l:oo!op orof sight, prostration of physical energy, 1;\\': asor\\'\\'business, study, or society, with dr. marston \\'01coax. men.o• heiti 0addressed specially to young 14. ...e oh 0%—marrlagerbanodbilgaltetipoltnsodanudetilrn°ooimp ten &c. \\' addressed to those who dam. 1010 . totof removing all impediments. qi thee dirpthaenß:orrrs4 2ar.cll\\'of healthful children, clearly expl.-11:91p .no. 3.—tiie great social evil. 11\\' wee voerdiseases which result from it, with da. minor ~,fc „i;,da. massrosi continues to be consulted pe ,e,:.„penned with.veoftiit.reatment, by which msactinr luso* a\\'\\'\\'s,y.o,,tifi,,:vilti9.residence,ez ens-......r5z 0 d-6 orby letter,47, eitherin at thexrßotal.lnsroittitatetßa: 1171,01\\'\"raftpaordrticeuiveargry.inf°\"\"ti°l4 70-00.',\n",
       " 'year_sep': '1859 [SEP] sitenirranv, roy institute as tou ry, v. ireno. i.—nervous debility, loss of kient.„2,l:oo!op orof sight, prostration of physical energy, 1;\\': asor\\'\\'business, study, or society, with dr. marston \\'01coax. men.o• heiti 0addressed specially to young 14. ...e oh 0%—marrlagerbanodbilgaltetipoltnsodanudetilrn°ooimp ten &c. \\' addressed to those who dam. 1010 . totof removing all impediments. qi thee dirpthaenß:orrrs4 2ar.cll\\'of healthful children, clearly expl.-11:91p .no. 3.—tiie great social evil. 11\\' wee voerdiseases which result from it, with da. minor ~,fc „i;,da. massrosi continues to be consulted pe ,e,:.„penned with.veoftiit.reatment, by which msactinr luso* a\\'\\'\\'s,y.o,,tifi,,:vilti9.residence,ez ens-......r5z 0 d-6 orby letter,47, eitherin at thexrßotal.lnsroittitatetßa: 1171,01\\'\"raftpaordrticeuiveargry.inf°\"\"ti°l4 70-00.',\n",
       " 'year_date': '1859 [DATE] sitenirranv, roy institute as tou ry, v. ireno. i.—nervous debility, loss of kient.„2,l:oo!op orof sight, prostration of physical energy, 1;\\': asor\\'\\'business, study, or society, with dr. marston \\'01coax. men.o• heiti 0addressed specially to young 14. ...e oh 0%—marrlagerbanodbilgaltetipoltnsodanudetilrn°ooimp ten &c. \\' addressed to those who dam. 1010 . totof removing all impediments. qi thee dirpthaenß:orrrs4 2ar.cll\\'of healthful children, clearly expl.-11:91p .no. 3.—tiie great social evil. 11\\' wee voerdiseases which result from it, with da. minor ~,fc „i;,da. massrosi continues to be consulted pe ,e,:.„penned with.veoftiit.reatment, by which msactinr luso* a\\'\\'\\'s,y.o,,tifi,,:vilti9.residence,ez ens-......r5z 0 d-6 orby letter,47, eitherin at thexrßotal.lnsroittitatetßa: 1171,01\\'\"raftpaordrticeuiveargry.inf°\"\"ti°l4 70-00.',\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4c0efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(len(data)*.2)\n",
    "train_test = data.train_test_split(test_size=test_size, seed=1984)\n",
    "test_set = train_test['test']\n",
    "val_size = int(len(train_test['train'])*.15)\n",
    "train_val =  train_test['train'].train_test_split(test_size=val_size, seed=1984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5f7f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 10200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['year', 'nlp', 'pol', 'loc', 'sentences', 'ocr', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "495b21a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/kbeelen/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/kbeelen/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/kbeelen/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /home/kbeelen/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /home/kbeelen/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/kbeelen/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/kbeelen/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /datadrive_2/bnert-hmd/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/datadrive_2/bnert-hmd\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /datadrive_2/bnert-hmd/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-hmd were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-hmd and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Didn't find file /datadrive_2/bnert-hmd/added_tokens.json. We won't load it.\n",
      "loading file /datadrive_2/bnert-hmd/vocab.txt\n",
      "loading file /datadrive_2/bnert-hmd/tokenizer.json\n",
      "loading file None\n",
      "loading file /datadrive_2/bnert-hmd/special_tokens_map.json\n",
      "loading file /datadrive_2/bnert-hmd/tokenizer_config.json\n",
      "loading configuration file /datadrive_2/bnert-time-st-y/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/datadrive_2/bnert-time-st-y\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30604\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file /datadrive_2/bnert-time-st-y/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-st-y were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-st-y and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file /datadrive_2/bnert-time-st-y/vocab.txt\n",
      "loading file /datadrive_2/bnert-time-st-y/tokenizer.json\n",
      "loading file /datadrive_2/bnert-time-st-y/added_tokens.json\n",
      "loading file /datadrive_2/bnert-time-st-y/special_tokens_map.json\n",
      "loading file /datadrive_2/bnert-time-st-y/tokenizer_config.json\n",
      "loading configuration file /datadrive_2/bnert-time-y/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/datadrive_2/bnert-time-y\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30523\n",
      "}\n",
      "\n",
      "loading weights file /datadrive_2/bnert-time-y/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file /datadrive_2/bnert-time-y/vocab.txt\n",
      "loading file /datadrive_2/bnert-time-y/tokenizer.json\n",
      "loading file /datadrive_2/bnert-time-y/added_tokens.json\n",
      "loading file /datadrive_2/bnert-time-y/special_tokens_map.json\n",
      "loading file /datadrive_2/bnert-time-y/tokenizer_config.json\n",
      "loading configuration file /datadrive_2/bnert-time-y_masked_25/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/datadrive_2/bnert-time-y_masked_25\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30523\n",
      "}\n",
      "\n",
      "loading weights file /datadrive_2/bnert-time-y_masked_25/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y_masked_25 were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y_masked_25 and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file /datadrive_2/bnert-time-y_masked_25/vocab.txt\n",
      "loading file /datadrive_2/bnert-time-y_masked_25/tokenizer.json\n",
      "loading file /datadrive_2/bnert-time-y_masked_25/added_tokens.json\n",
      "loading file /datadrive_2/bnert-time-y_masked_25/special_tokens_map.json\n",
      "loading file /datadrive_2/bnert-time-y_masked_25/tokenizer_config.json\n",
      "loading configuration file /datadrive_2/bnert-time-y_masked_75/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/datadrive_2/bnert-time-y_masked_75\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30523\n",
      "}\n",
      "\n",
      "loading weights file /datadrive_2/bnert-time-y_masked_75/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-time-y_masked_75 were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-time-y_masked_75 and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file /datadrive_2/bnert-time-y_masked_75/vocab.txt\n",
      "loading file /datadrive_2/bnert-time-y_masked_75/tokenizer.json\n",
      "loading file /datadrive_2/bnert-time-y_masked_75/added_tokens.json\n",
      "loading file /datadrive_2/bnert-time-y_masked_75/special_tokens_map.json\n",
      "loading file /datadrive_2/bnert-time-y_masked_75/tokenizer_config.json\n",
      "loading configuration file /datadrive_2/bnert-pol-st/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/datadrive_2/bnert-pol-st\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30528\n",
      "}\n",
      "\n",
      "loading weights file /datadrive_2/bnert-pol-st/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-pol-st were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-pol-st and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file /datadrive_2/bnert-pol-st/vocab.txt\n",
      "loading file /datadrive_2/bnert-pol-st/tokenizer.json\n",
      "loading file /datadrive_2/bnert-pol-st/added_tokens.json\n",
      "loading file /datadrive_2/bnert-pol-st/special_tokens_map.json\n",
      "loading file /datadrive_2/bnert-pol-st/tokenizer_config.json\n",
      "loading configuration file /datadrive_2/bnert-pol/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/datadrive_2/bnert-pol\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30523\n",
      "}\n",
      "\n",
      "loading weights file /datadrive_2/bnert-pol/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /datadrive_2/bnert-pol were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /datadrive_2/bnert-pol and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file /datadrive_2/bnert-pol/vocab.txt\n",
      "loading file /datadrive_2/bnert-pol/tokenizer.json\n",
      "loading file /datadrive_2/bnert-pol/added_tokens.json\n",
      "loading file /datadrive_2/bnert-pol/special_tokens_map.json\n",
      "loading file /datadrive_2/bnert-pol/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "checkpoints = [('distilbert','distilbert-base-uncased','[SEP]','year_sep'),\n",
    "               ('hmd_distilbert','/datadrive_2/bnert-hmd','[SEP]','year_sep'),\n",
    "               ('bnert-time-st-y','/datadrive_2/bnert-time-st-y','[SEP]','st_year_sep'),\n",
    "               ('bnert-time-y','/datadrive_2/bnert-time-y','[DATE]','year_date'),\n",
    "               ('bnert-time-y_masked_25','/datadrive_2/bnert-time-y_masked_25','[DATE]','year_date'),\n",
    "               ('bnert-time-y_masked_75','/datadrive_2/bnert-time-y_masked_75','[DATE]','year_date'),\n",
    "               ('bnert-pol-st','/datadrive_2/bnert-pol-st','[SEP]','year_sep'),\n",
    "               ('bnert-pol','/datadrive_2/bnert-pol','[SEP]','year_sep')]\n",
    "\n",
    "model_dict = defaultdict(dict)\n",
    "for name,checkpoint, st, sent_col in checkpoints:\n",
    "    model_dict[name]['model'] = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=num_labels)\n",
    "    model_dict[name]['tokenizer'] = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    #model_dict[name]['special_token'] = st\n",
    "    model_dict[name]['sentences'] = sent_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44f12775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['year', 'pol', 'sentences', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 10200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['year', 'pol', 'sentences', 'st_year_sep', 'year_sep', 'year_date', 'label'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val = train_val.remove_columns(['nlp', 'ocr', 'loc'])\n",
    "train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6fed10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def add_text_col(example,source):\n",
    "#    return {'text' : example[source]}\n",
    "\n",
    "def preprocess_function(examples, target_col):\n",
    "    return tokenizer(examples[target_col], truncation=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07e895a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for distilbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b134e4555b41a5950e7b123e414387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6f940db4f04ea2bc3b9fdbd605e02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, year_date, st_year_sep, year. If pol, sentences, year_sep, year_date, st_year_sep, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 12:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.661400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.529900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.445600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.211800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_distilbert/checkpoint-500\n",
      "Configuration saved in ./results_distilbert/checkpoint-500/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-1000\n",
      "Configuration saved in ./results_distilbert/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-1500\n",
      "Configuration saved in ./results_distilbert/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-2000\n",
      "Configuration saved in ./results_distilbert/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-2500\n",
      "Configuration saved in ./results_distilbert/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_distilbert/checkpoint-3000\n",
      "Configuration saved in ./results_distilbert/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_distilbert/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_distilbert/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_distilbert/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /datadrive_2/distilbert-pol/config.json\n",
      "Model weights saved in /datadrive_2/distilbert-pol/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/distilbert-pol/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/distilbert-pol/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c751c51bd5fe4ebeae66777245603f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp. If pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for hmd_distilbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5aa75665d2f487ca93d848ca2927a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5098735ca122496db6ff8c321a9f8413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, year_date, st_year_sep, year. If pol, sentences, year_sep, year_date, st_year_sep, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 12:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.472400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.385200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.315800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.238200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.184600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-500\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-500/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-1000\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-1500\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-2000\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-2500\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_hmd_distilbert/checkpoint-3000\n",
      "Configuration saved in ./results_hmd_distilbert/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_hmd_distilbert/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_hmd_distilbert/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_hmd_distilbert/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /datadrive_2/hmd_distilbert-pol/config.json\n",
      "Model weights saved in /datadrive_2/hmd_distilbert-pol/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/hmd_distilbert-pol/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/hmd_distilbert-pol/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9f078a7eb14936b40f09fc4a884874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp. If pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-st-y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0bd38c281449599c8528ebc0433453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52678bca0a34c54b8c537ebd432b925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, year_date, st_year_sep, year. If pol, sentences, year_sep, year_date, st_year_sep, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 12:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-500\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-st-y/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-time-st-y/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-time-st-y/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-st-y/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-st-y/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /datadrive_2/bnert-time-st-y-pol/config.json\n",
      "Model weights saved in /datadrive_2/bnert-time-st-y-pol/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/bnert-time-st-y-pol/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/bnert-time-st-y-pol/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a910c0102a0c464a96065b8847d09848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp. If pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42759588c1eb462385ad2204a9d0e5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0be62f7afb4dbb8bc6d07b7d8d961d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, year_date, st_year_sep, year. If pol, sentences, year_sep, year_date, st_year_sep, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 13:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.632100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.444400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.364700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.146200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-500\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-time-y/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-time-y/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /datadrive_2/bnert-time-y-pol/config.json\n",
      "Model weights saved in /datadrive_2/bnert-time-y-pol/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/bnert-time-y-pol/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/bnert-time-y-pol/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8a2ad0845b4864ba3a535caff887b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp. If pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-y_masked_25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779ad95beac84bafbaf1a19e06c98be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9454d2ad6a4f9aab4af132419bb4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, year_date, st_year_sep, year. If pol, sentences, year_sep, year_date, st_year_sep, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 13:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.627400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.153800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-500\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_25/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-time-y_masked_25/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_25/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_25/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_25/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /datadrive_2/bnert-time-y_masked_25-pol/config.json\n",
      "Model weights saved in /datadrive_2/bnert-time-y_masked_25-pol/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/bnert-time-y_masked_25-pol/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/bnert-time-y_masked_25-pol/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b8785f87e94b5ead8a40f733525ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp. If pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-time-y_masked_75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325eccaec70c4704a2e8d5efc7b30b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef45171c438b486ba3539dcbf7cf88f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, year_date, st_year_sep, year. If pol, sentences, year_sep, year_date, st_year_sep, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 13:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.629300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.145100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-500\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-time-y_masked_75/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-time-y_masked_75/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-time-y_masked_75/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-time-y_masked_75/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-time-y_masked_75/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /datadrive_2/bnert-time-y_masked_75-pol/config.json\n",
      "Model weights saved in /datadrive_2/bnert-time-y_masked_75-pol/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/bnert-time-y_masked_75-pol/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/bnert-time-y_masked_75-pol/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6366aa4648403892afff92d5ee973d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp. If pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-pol-st\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31242d0956954a9eade95920afa685da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4f098fe8ec494589066456aa53942f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, year_date, st_year_sep, year. If pol, sentences, year_sep, year_date, st_year_sep, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 13:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.178200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-500\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol-st/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-pol-st/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-pol-st/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol-st/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol-st/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /datadrive_2/bnert-pol-st-pol/config.json\n",
      "Model weights saved in /datadrive_2/bnert-pol-st-pol/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/bnert-pol-st-pol/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/bnert-pol-st-pol/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bda70a4a35c440ca19490da480e0222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp. If pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a model for bnert-pol\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a916effc284e12a7bdfcd6e2894a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9896e08ad64661ac61186b6ea09c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, year_date, st_year_sep, year. If pol, sentences, year_sep, year_date, st_year_sep, year are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/datadrive_2/lm2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3190' max='3190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3190/3190 14:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.159700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-500\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-500/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-1000\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-1000/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-1500\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-1500/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-2000\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-2000/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-2500\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-2500/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results_bnert-pol/checkpoint-3000\n",
      "Configuration saved in ./results_bnert-pol/checkpoint-3000/config.json\n",
      "Model weights saved in ./results_bnert-pol/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_bnert-pol/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results_bnert-pol/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /datadrive_2/bnert-pol-pol/config.json\n",
      "Model weights saved in /datadrive_2/bnert-pol-pol/pytorch_model.bin\n",
      "tokenizer config file saved in /datadrive_2/bnert-pol-pol/tokenizer_config.json\n",
      "Special tokens file saved in /datadrive_2/bnert-pol-pol/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774922d86df44497b45d18634666d7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp. If pol, sentences, year_sep, ocr, loc, st_year_sep, year_date, year, nlp are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_dict = defaultdict(dict)\n",
    "\n",
    "for name, mdict in model_dict.items():\n",
    "    print(f'Creating a model for {name}')\n",
    "    tokenizer = model_dict[name]['tokenizer']\n",
    "    model = model_dict[name]['model']\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    sent_col = model_dict[name]['sentences']\n",
    "    \n",
    "    #train_val = train_val.map(add_text_col,fn_kwargs={'source': sent_col})\n",
    "    train_val = train_val.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "    seed=1984,\n",
    "    output_dir=f\"./results_{name}\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_val[\"train\"],\n",
    "    eval_dataset=train_val[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    model.save_pretrained(f'/datadrive_2/{name}-pol')\n",
    "    tokenizer.save_pretrained(f\"/datadrive_2/{name}-pol\")\n",
    "    \n",
    "    test_set = test_set.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "    predictions = trainer.predict(test_set)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    result_dict[name]['f1_macro'] = f1_score(preds,predictions.label_ids,average='macro')\n",
    "    result_dict[name]['f1_micro'] = f1_score(preds,predictions.label_ids,average='micro')\n",
    "    result_dict[name]['accuracy']  = accuracy_score(preds,predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52123841",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(result_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60d00595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  f1\\_macro &  f1\\_micro &  accuracy \\\\\n",
      "\\midrule\n",
      "distilbert             &     0.631 &     0.800 &     0.800 \\\\\n",
      "hmd\\_distilbert         &     0.663 &     0.828 &     0.828 \\\\\n",
      "bnert-time-st-y        &     0.670 &     0.820 &     0.820 \\\\\n",
      "bnert-time-y           &     0.667 &     0.830 &     0.830 \\\\\n",
      "bnert-time-y\\_masked\\_25 &     0.671 &     0.831 &     0.831 \\\\\n",
      "bnert-time-y\\_masked\\_75 &     0.672 &     0.831 &     0.831 \\\\\n",
      "bnert-pol-st           &     0.668 &     0.827 &     0.827 \\\\\n",
      "bnert-pol              &     0.662 &     0.825 &     0.825 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_397981/2842510611.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(results_df.round(3).to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(results_df.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61ebcd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('tables/classsify_pol_regex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b78a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm2",
   "language": "python",
   "name": "lm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
